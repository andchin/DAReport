<<initialActions, eval=TRUE, echo=FALSE, term=FALSE>>=
options(prompt=" ", continue = " ", concordance=FALSE)
require(car)
require(mlbench)
require(MASS)
require(gplots)
data(Glass)
Glass.type1 = Glass[which(Glass$Type == 1), -10]
Glass.type2 = Glass[which(Glass$Type == 2), -10]
Glass.type3 = Glass[which(Glass$Type == 3), -10]
Glass.type5 = Glass[which(Glass$Type == 5), -10]
Glass.type6 = Glass[which(Glass$Type == 6), -10]
Glass.type7 = Glass[which(Glass$Type == 7), -10]

Glass.type1s = Glass[which(Glass$Type == 1), -10][,-9][,-8]
Glass.type2s = Glass[which(Glass$Type == 2), -10][,-9][,-8]
Glass.type3s = Glass[which(Glass$Type == 3), -10][,-9][,-8]
Glass.type5s = Glass[which(Glass$Type == 5), -10][,-9][,-8]
Glass.type6s = Glass[which(Glass$Type == 6), -10][,-9][,-8][,-6]
Glass.type7s = Glass[which(Glass$Type == 7), -10][,-9][,-6][,-3]
@
%%%%%
<<chisqTestFunctions, eval=TRUE, echo=FALSE, term=FALSE>>=
### Chi squared test ###

# Calculates bounds of bins (classes) of a data sample.
# The initial bounds are given by initial_breaks, k denotes the minimum class size.
makebins = function(data, initial_breaks, k) {
  h = hist(data, breaks=initial_breaks, plot=FALSE)
  
  br = h$breaks
  changed = TRUE
  
  while(changed) {
    h = hist(data, breaks=br, plot=FALSE)
    br = h$breaks
    changed=FALSE
    
    for(i in 1:length(h$counts)) {
      if(h$counts[i] < k) {
        if(i > 1 && i < length(h$counts)) {
          if(h$counts[i-1] < h$counts[i+1]) {
            br = br[-i]
            changed = TRUE
            break
          }
          else {
            br = br[-(i+1)]
            changed = TRUE
            break
          }
        }
        # index on first class
        else if(i == 1) {
          br = br[-2]
          changed = TRUE
          break
        }
        # index on last class
        else {
          br = br[-(length(h$counts))]
          changed = TRUE
          break
        }
      }
    }
  }
  return(br)
}

# Calculates the expected probabilities of a normal distribution with the given parameters mean and sd
# for the given bin (class) bounds
probabilities.exp = function(bins, mean, sd) {
  result = rep(0, length(bins)-1)
  
  result[1] = pnorm(q=bins[2], mean=mean, sd=sd)
  
  for(i in 2:(length(bins)-1)) {
    result[i] <- pnorm(q=bins[i+1], mean=mean, sd=sd) - pnorm(q=bins[i], mean=mean, sd=sd)
  }
  
  result[length(bins)-1] = pnorm(q=bins[length(bins)-1], mean=mean, sd=sd, lower.tail=FALSE)
  
  return(result)
}

# Returns the chi squared test statistics for the given actual and expected values.
teststat.chi = function(actual, expected) {
  sum((actual - expected)^2 / expected)
}

# Performs a chi squared goodness of fit test on the given data for the assumption of a normal distribution.
# The parameters are estimated from the sample.
# The initial bounds for the classes are given by initial_breaks, min denotes the minimum class size.
# The significance level is determined by sig.
chisq.test.norm = function(data, initial_breaks, min, sig) {
  bins = makebins(data, initial_breaks, min)
  hist = hist(data, breaks=bins, plot=FALSE)
  expected_probabilities = probabilities.exp(bins, mean(data), sd(data))
  expected_frequencies = expected_probabilities * length(data)
  teststat = teststat.chi(hist$counts, expected_frequencies)
  df=length(bins)-4 # df: length(bin) - 1 classes, 2 estimated parameters (mean, sd)
  critical_value = ifelse(df < 1, NA, qchisq(p=1-sig, df=df))
  
  return(list(hist = hist,
              expected_probabilities = expected_probabilities,
              expected_frequencies = expected_frequencies,
              teststat = teststat,
              critical_value = critical_value,
              p_value = ifelse(df < 1, NA, 1 - pchisq(q=teststat, df=df)),
              rejected = teststat > critical_value))
}

chisq.test.norm.eh = function(data, initial_breaks, min, sig) {
  res = tryCatch(chisq.test.norm(data, initial_breaks, min, sig), error=function(e) NA)
  if(is.na(res)) {
    return(list(hist = NA,
                expected_probabilities = NA,
                expected_frequencies = NA,
                teststat = NA,
                critical_value = NA,
                p_value = NA,
                rejected = NA))
  }
  return(res)
}

# Returns class centers (mids) for given breaks.
getMids = function(breaks) {
  mids = rep(0, length(breaks)-1)
  for(i in 1:(length(breaks)-1)) {
    mids[i] = (breaks[i+1] + breaks[i]) / 2
  }
  return(mids)
}

# Creates artificial data from expected frequencies as input for hist().
# testresult is the result of chisq.test.norm.
# mult is the factor by which the frequencies are multiplied (to increase accuracy
# after rounding and casting to integer).
dataFromFreq = function(testresult, mult) {
  x = testresult$expected_frequencies
  mids = getMids(testresult$hist$breaks)
  freqs = as.integer(round(mult*x))
  result = rep(0, sum(freqs))
  counter = 1
  for(i in 1:length(freqs)) {
    for(j in 1:freqs[i]) {
      result[counter] = mids[i]
      counter = counter + 1
    }
  }
  return(result)
}

# Returns a histogram of the expected frequencies from testresult (result of chisq.test.norm).
# Does not plot the histogram.
histFromExpectedFreqs = function(testresult) {
  hist(dataFromFreq(testresult, 1000), breaks=testresult$hist$breaks, freq=FALSE, plot=FALSE)
}
@
%%%%%
<<boxcoxFunctions, eval=TRUE, echo=FALSE, term=FALSE>>=
boxcox = function(x, lambda) {
  if(lambda == 0) {
    return(log(x))
  }
  return((x^lambda - 1) / lambda)
}

boxcox.likelihood = function(X, lambda) {
  n = length(X)
  xlmean = 1/n * sum(boxcox(X, lambda))
  return(
    -n/2 * log(1/n * sum((boxcox(X, lambda) - xlmean)^2)) + (lambda-1) * sum(log(X))
  )
}

powerTransform.eh = function(X) {
  tryCatch(powerTransform(X), error=function(e) NULL)
}

transformDataFrame = function(data) {
  data.lambda = apply(data, 2, powerTransform.eh)
  data.t = data
  for(name in colnames(data.t)) {
    if(is.null(data.lambda[[name]])) {
      col = rep(NA, length(data.t[,1]))
    }
    else {
      col = boxcox(data[[name]], data.lambda[[name]][["lambda"]])
    }
    data.t[[name]] = col
  }
  return(data.t)
}

transformVector = function(data) {
  lambda = powerTransform(data)
  return(boxcox(data, lambda$lambda))
}
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Template for the papers to the case studies of Data Analytics
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[a4paper, 12pt, titlepage, headsepline, listof = totoc, bibliography = totoc, numbers = noenddot]{scrartcl}
\usepackage[left = 3cm, right = 2cm, top = 2.2cm, bottom = 3.5cm]{geometry} % spaces on the sides of the paper
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{natbib}
\usepackage{scrpage2}
\usepackage{dsfont}
\usepackage{float}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{chngcntr}

\frenchspacing

\usepackage{bbm}
\usepackage[labelfont=bf, font=footnotesize, tableposition=top]{caption}
\DeclareCaptionType[fileext=los,placement={!ht}]{listing}
%\usepackage{mdframed}
\usepackage{boxedminipage}
%\usepackage{chngcntr}
%\def \lstWidth {0.9}

\counterwithin{figure}{section}

\counterwithin{table}{section}

\newcommand{\eg}{e.\,g. }
\newcommand{\ie}{i.\,e. }
\newcommand{\cdf}{c.\,d.\,f. }
\newtheorem{thm}{Theorem}
\newtheorem{df}{Definition}
\newtheorem{lem}{Lemma}

%% path, where the figures are stored
\graphicspath{{./images/}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% design of the head of the report pages

\clearscrheadings                   	% clears all predefined formats voreingestellte Formatierungen
\pagestyle{scrheadings}			% use this style only on the actual text
\ohead{}		% writes your name on each side in the upper right corner
\automark{section}                  
\ihead{\headmark}				% automatically writtes the section name in the upper left corner
\cfoot{\pagemark}				% page number on the bottom (center)


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% cover sheet

\title{\hrulefill \\ \vspace*{1cm} Case Studies\\\vspace*{0.5cm}
 "Data Analytics" \\ \vspace*{1cm}\hrulefill\vspace*{1.5cm}}
\subtitle{Topic\\\vspace*{1.5cm} Summer Term 2013\vspace*{1.5cm}}
\author{Andrey Chinnov, Sebastian Honermann, Carlos Zydorek}
%\institute{e-mail}

\begin{document}
\SweaveOpts{concordance=TRUE}
\thispagestyle{empty}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% table of contents

\thispagestyle{empty}
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% the document itself

\newpage
\setcounter{page}{1}
\section{Introduction}

\subsection{Normality as a requirement for statistical methods}

Since the normal distribution has some convenient analytical properties, many statistical tests (\eg Two-sample z-test, One-sample t-test, or Chi-squared test for variance) are based on the assumption that data were drawn from a normal distribution. Furthermore, the central limit theorem for example is applied in many cases in practice. It states that, when under certain conditions a large number of observations stem from the same distribution, their mean will be approximately normally distributed. This statement holds independent of the original distribution the observations are drawn from. Hence, methods for testing sample data on normality are of major importance. 

This report aims at describing test methods for normality and conducting them on the Glass data set. Since the hypothesis of normality is rejected for most of the variables, this report is focused on methods for testing on univariate normal distribution. A transformation is applied to convert the data so that it complies with a normal distribution if possible. Nevertheless, a small part of this report deals with multivariate normal distribution.

Test methods for univariate normal distribution are introduced in section \ref{sec:preliminaries}. Furthermore, a transformation method and a technique for identifying multivariate (more precisely: bivariate) normally distributed data are described. In section \ref{sec:testing}, the introduced test methods are conducted on the original data as well as on the transformed data. Subsequently, the technique for checking on bivariate normal distribution is demonstrated on an example from the Glass data set. The findings are shortly concluded in section \ref{sec:conclusion}.

\subsection{The Glass data set}

The data set that is investigated in this report is the \texttt{Glass} data set from the R package \texttt{mlbench}, originally taken from the UCI Repository of Machine Learning Databases (Bache, K., Lichman, M. 2013). It contains 214 observations on 10 variables of glass specimen. The first variable is the refractive index (RI) followed by weight percent in the corresponding oxide of Sodium (Na), Magnesium (Mg), Aluminium (Al), Silicon (Si), Potassium (K), Calcium (Ca), Barium (Ba) and Iron (Fe). One variable specifies the type of glass as a class attribute. There are seven different types of glass but only six of them are contained in the present data set.

\newpage
\section{Preliminaries}\label{sec:preliminaries}

\subsection{The Normal Distribution}

The (univariate) normal distribution $N(\mu,\sigma^2)$ is defined to have the density

\[ Pr(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp (-\frac{1}{2\sigma^2}(x - \mu)^2) \]

where $\mu \in \mathbb{R}$ and $\sigma > 0$.

The univariate normal distribution is generalised by the multivariate normal distribution for higher
dimensions. $X$ has a multivariate normal distribution with mean vector $\mu$ and covariance $\Sigma$ > 0, i.e. $X \sim N_p(\mu,\Sigma)$, if

\[Pr(x) = |2\pi\Sigma|^{-1/2} \,\mbox{exp}\left\{-\frac{1}{2}(x-\mu)' \Sigma^{-1}(x-\mu)\right\}\]

Contour lines of the plot of a multivariate normal distribution are shaped elliptically. Those ellipsoids are centered at $\mu : \left\{x:(x-\mu)'\, \Sigma^{-1}(x-\mu) = c^2\right\}$ with some constant $c$.

A first approach to check a sample of several variables on multivariate normal distribution is to examine the plot on the appearance of elliptical contour lines. An exemplary contour plot is depicted in figure \ref{fig:plotMVNSim}. The data is simulated from the mulitvariate normal distribution with the parameters
\[\mu = \left(\begin{array}{c} 1\\ 2\end{array} \right), \quad \Sigma = \left[\begin{array}{cc} 27 & 15\\ 15 & 18\end{array} \right]\]
and a sample size of 500. The contour lines for this simulated sample are apparently elliptically shaped. Obviously, this graphical method only works for two variables respectively, \ie for bivariate distributions.

\begin{figure}[h!]
\centering
<<plotMVNSim, eval=TRUE, echo=FALSE, term=FALSE, fig=TRUE, include=FALSE>>=
set.seed=1337
mvns = mvrnorm(n=500, c(1,2), matrix(c(27,15,15,18), nrow=2))
X <- mvns[,1]
Y <- mvns[,2]
ci2d(X, Y, show="contour", col="black", show.points=TRUE, factor=3)
@
\includegraphics[width=0.8\textwidth]{report-plotMVNSim}
\caption{Data plot with contour lines of a simulated sample from a bivariate normal distribution}
\label{fig:plotMVNSim}
\end{figure}

\subsection{Test methods for normality}\label{sec:methods}

A statistical hypothesis test which tests empirical data on conformance with a certain distribution (or a family of distributions) is called a goodness of fit test. The null hypothesis usually refers to the position that the tested sample has been drawn from a population which is distributed according to the given distribution. Consequently, the alternative hypothesis states that the sample was drawn from a population of any other distribution. In every test, a certain method is used to calculate a test statistic from the data. If the test statistic exceeds a critical value which is computed for the particular distribution at a certain significance level, the null hypothesis is rejected. The p-value is the lowest significance level for which the null hypothesis would still be rejected. It can be interpreted as the probability of getting a result like the present one or an even more extreme result if the null hypothesis is true.

\subsubsection{Q-Q-plot}\label{sec:qq-theoretical}

Quantile-Quantile-Plots provide a graphical comparison of the quantiles of two probability distributions. The observed values are plotted against the quantiles of a theoretical distribution. To check for normality, the observations $x_1, \dots, x_n$ from a sample with size $n$ are ordered ( $x_{(1)} \le x_{(2)} \le \dots \le x_{(n)}$ ) and plotted against an assumed cumulative distribution function. Let $x_{(i)}$ denote the sample quantiles. For each sample quantile $x_{(j)}$ with $j$ observations to the left, the proportion of these $j$ observations is approximated by
\[ p_{(j)} = \frac{j - \frac{1}{2}} {n}\]
and the theoretical quantiles $q_{(j)}$ are defined by
\[ q_{(j)} = \Phi^{-1}(p_{(j)}) \]

where $\Phi$ is the cumulative distribution function of the standard normal distribution.

In the QQ-Plot these theoretical quantiles $q_{(j)}$ are then plotted against the sample quantiles $x_{(j)}$. Given a sufficient sample size, in the case of normally distributed data the observed quantiles will be linearly related to the theoretical normal distribution quantiles in the QQ-plot.

\subsubsection{Shapiro-Wilk test}\label{sec:shapiro-theoretical}

The Shapiro-Wilk test is a statistical procedure for testing a complete sample for normality. Basically, the test statistic, denoted as $W$, indicates correlation of the observed quantile values with the assumed cumulative distribution function quantiles and is therefore close to the idea of QQ-plots or probability-plots. 

\[ W = \frac{\sum \limits_{i=1}^n (a_i x_i)^2} {\sum \limits_{i=1}^n (x_i-\bar{x})^2}\]
where

$a_i$ denotes the normalized "best linear unbiased" coefficients and

$x_i$ denotes the ordered observations.

In detail, $W$ is obtained by dividing the best linear estimate of the slope of a linear regression of the ordered observations $x_i$ by the actual variance of the ordered observed sample values.

The resulting test statistic $W$ is then compared with a critial $W$-value for a given sample size $n$. The resulting significance level can be looked up in existing tables, however, most statistic software (including R) automatically determines the according p-value via the Monte Carlo method when displaying the test statistic $W$.

In general, the statistical power of tests decreases with smaller sample sizes. Compared to other statistical tests, like the Kolmogorov-Smirnov test or the Chi-Squared-Test, the Shapiro-Wilk test's statistical power decreases less with smaller samples. Since 4 of 6 glass type samples (glass type 3, 5, 6, and 7) have a size of $n$ < 50, the test result is quite interesting for the tests of the subdatasets.

In order to conduct the Shapiro-Wilk test in R the function \texttt{shapiro.test()} from the package \texttt{stats} is used.

In some cases all observations of certain variables are identical. In these cases the Shapiro-Wilk-Statistic is not applicable for these specific variables since the expression 
\[ \sum \limits_{i=1}^n (y_i - \bar{y})^2 \]
in the denominator, denoting the variance, sums up to 0. Tests can therefore only be run if such variables are excluded from the testing procedure.

\subsubsection{Pearson's chi-squared test}\label{sec:chisq-theoretical}
Pearson's chi-squared goodness of fit test is used to test whether data from a sample are distributed according to an arbitrary theoretical distribution. The main idea of this test is to divide the observations $x_1, \dots, x_n$ into several pairwise disjoint classes $C_1, \dots, C_K$ and compare the empirical frequencies within these classes to the theoretical frequencies, which are expected if the data complies to the hypothetical distribution.
\begin{figure}[h!]
<<chisqSampleHist, eval=TRUE, echo=FALSE, term=FALSE, fig=TRUE, include=FALSE>>=
testres = chisq.test.norm(Glass$Na, 30, 5, 0.01)
par(mfrow=c(2,2))
hist=testres$hist
plot(hist, col=rgb(1, 0, 0,0.5), main="Histogram of\n empirical data", xlab="empirical data")
hist.exp=histFromExpectedFreqs(testres)
plot(hist.exp, col=rgb(0, 0, 1,0.5), main="Histogram of\n expected densities", xlab="expected data")
plot(hist, col=rgb(1, 0, 0,0.5), main="Histogram of both empirical\n data and expected densities", xlab="empirical and expected data")
plot(hist.exp, col=rgb(0, 0, 1,0.5), add=TRUE)
par(mfrow=c(1,1))
@
\includegraphics[width=\textwidth]{report-chisqSampleHist}
\caption{Exemplary histrograms of a data sample, expected densities for a normal distribution with parameters estimated from the sample and a combined histogram of these both histograms.}
\label{fig:chisqSampleHist}
\end{figure}
If the histograms of the sample data and the expected densities are plotted together (see figure \ref{fig:chisqSampleHist}), the area of density that is not overlapped by both histograms can be understood as a kind of indicator for the likelihood that the sample is drawn from a population which is distributed according to the hypothetical distribution: The larger the non-overlapping area, the less likely it is that the sample is drawn from a population with the assumed distribution. However, the test statistic of the chi-squared test is calculated differently, namely by the sum of the squared differences between observed frequencies $O_k$ and expected frequencies $E_k$ divided by the expected frequencies for each class $k$ of the overall $K$ classes. Thus, the test statistic is calculated by
\[\chi^2 = \sum_{k=1}^{K}\frac{(O_k - E_k)^2}{E_k}\]
Larger differences of observed and expected values indicate a lower compliance to the assumed distribution. However, the addends are not weighted (neither by the size of a class nor by the frequencies within a class nor by any other means). Therefore, the class bounds should be chosen equidistant or in such a way that the classes contain preferably the same number of observations, or should be chosen according to similar reasonable rationales. The test statistic is approximately $\chi^2$-distributed with $K-1$ degrees of freedom -- the larger the sample size, the better the approximation. A sample size that is too small can be a reason for the approximation being insufficient. Moreover, for each parameter of the hypothetical distribution which is estimated from the data sample, one degree of freedom is lost; the number of estimated parameters is denoted by $p$. The test statistic is determined under the null hypothesis that the sample is distributed according to the assumed distribution and the chi-squared test is defined as
%\[\phantom{\quad \textrm{with}\ F=\chi^2_{K-1-p}} \delta(Y) = \mathbbm{1}_{\{\chi^2\, >\, F^{-1}(1-\alpha)\}} \quad \mbox{with}\ F=\chi^2_{K-1-p}\]
\[
  \phantom{\quad\mbox{with}\ F=\chi^2_{K-1-p}}
  \delta(Y) =
   \left\{ 
    \begin{array}{cll}
%\vspace{12pt}
                 1 & \mbox{if} \ \chi^2 > F^{-1}(1-\alpha)\\
                 0 & \mbox{otherwise}
    \end{array} 
   \right.
   \quad\mbox{with}\ F=\chi^2_{K-1-p}
\]
for a given significance level $\alpha$ where $Y$ is a multinomial distributed random variable denoting the counts of observations in each class with $Y_k = |\{i : X_i \in C_k\}|$.

A common requirement for a sufficient approximation demands the minimum number 
of observations in each class not to fall below five (Steel, R.G.D., Torrie, J.H. 1960, p. 350). 
Hence, marginal or even inner classes have to be unified in some cases in order to achieve a 
sufficient class size. 
%The following R-function is used here for this purpose.
<<makebinsLst, eval=FALSE, echo=FALSE>>=
# Calculates bounds of bins (classes) of a data sample.
# The initial bounds are given by initial_breaks,
# k denotes the minimum class size.
makebins = function(data, initial_breaks, k) {
  h = hist(data, breaks=initial_breaks, plot=FALSE)
  
  br = h$breaks
  changed = TRUE
  
  while(changed) {
    h = hist(data, breaks=br, plot=FALSE)
    br = h$breaks
    changed=FALSE
    
    for(i in 1:length(h$counts)) {
      if(h$counts[i] < k) {
        if(i > 1 && i < length(h$counts)) {
          if(h$counts[i-1] < h$counts[i+1]) {
            br = br[-i]
            changed = TRUE
            break
          }
          else {
            br = br[-(i+1)]
            changed = TRUE
            break
          }
        }
        # index on first class
        else if(i == 1) {
          br = br[-2]
          changed = TRUE
          break
        }
        # index on last class
        else {
          br = br[-(length(h$counts))]
          changed = TRUE
          break
        }
      }
    }
  }
  return(br)
}
@
%Further functions are needed for calculating the expected frequencies, the test
% statistic, and the result of the test (since the mean and the standard deviation are estimated from the sample, two degrees of freedom are additionally lost):
<<chisqTestLst, eval=FALSE, echo=FALSE>>=
# Calculates the expected probabilities of a normal distribution
# with the given parameters mean and sd
# for the given bin (class) bounds
probabilities.exp = function(bins, mean, sd) {
  result = rep(0, length(bins)-1)
  
  result[1] = pnorm(q=bins[2], mean=mean, sd=sd)
  
  for(i in 2:(length(bins)-1)) {
    result[i] <- pnorm(q=bins[i+1], mean=mean, sd=sd)
      - pnorm(q=bins[i], mean=mean, sd=sd)
  }
  
  result[length(bins)-1] = pnorm(q=bins[length(bins)-1],
    mean=mean, sd=sd, lower.tail=FALSE)
  
  return(result)
}

# Returns the chi squared test statistics
# for the given actual and expected values.
teststat.chi = function(actual, expected) {
  sum((actual - expected)^2 / expected)
}

# Performs a chi squared goodness of fit test on the given data
# for the assumption of a normal distribution.
# Returns true if the null hypothesis (sample drawn from a
# normal distributed population) is rejected, false otherwise.
# The parameters are estimated from the sample.
# The initial bounds for the classes are given by initial_breaks,
# min denotes the minimum class size.
# The significance level is determined by sig.
chisq.test.norm = function(data, initial_breaks, min, sig) {
  bins = makebins(data, initial_breaks, min)
  hist = hist(data, breaks=bins, plot=FALSE)
  expected_probabilities = probabilities.exp(bins, mean(data), sd(data))
  expected_frequencies = expected_probabilities * length(data)
  teststat = teststat.chi(hist$counts, expected_frequencies)
  
  # length(bin) - 1 classes, 2 estimated parameters (mean, sd)
  df=length(bins)-4
  critical_value = ifelse(df < 1, NA, qchisq(p=1-sig, df=df))
  
  print(teststat > critical_value)
  
  return(list(hist = hist,
              expected_probabilities = expected_probabilities,
              expected_frequencies = expected_frequencies,
              teststat = teststat,
              critical_value = critical_value,
              p_value =
                ifelse(df < 1, NA, 1 - pchisq(q=teststat, df=df)),
              rejected = teststat > critical_value))
}
@
A drawback of Pearson's chi-squared test is its inconsistency caused by information reduction, \ie information about the data sample is lost in the process of categorising the observations in classes. As a consequence, different class bounds can lead to different test results. Furthermore, this test is rather suited for large sample sizes.

\subsubsection{Kolmogorov-Smirnov test}\label{sec:kolm-smir}
Like the other tests introduced in the previous chapters, the Kolmogorov-Smirnov (subsequently abbreviated as KS) is used for testing whether a given univariate sample $x=(x_1,x_2 \dots x_n)$ with unknown distribution $\mathbb{P}$ is distributed according to a completely determined distribution $\mathbb{P}_0$. Based on the test a decision is made between the following two hypotheses:
\[\begin{array}{rcl}
H_0 & : & \mathbb{P} = \mathbb{P}_0,\\
H_1 & : & \mathbb{P} \ne \mathbb{P}_0.
\end{array}\]
This decision is made according to the KS test
statistics and a given significance level $\alpha$.

\begin{df}
For a given univariate sample $x=(x_1, x_2, \dots, x_n)$ the function
\[F_n=\frac{1}{n}\sum_{i=1}^n \mathbbm{1}_{\{x_i\le x\}}\] is called empirical
cumulative distribution function (\cdf), where $\mathbbm{1}_{\{x_i\le x\}}$ is an
indicator function defined as follows: $\mathbbm{1}_{\{x_i\le x\}}(x)=\left\lbrace 
\begin{array}{cll}
                 1 & \mbox{if} \ x_i\le x\\
                 0 & \mbox{otherwise}.
\end{array} 
\right.$
\end{df}
The exemplary graph of such a function is depicted in the Figure
\ref{fig:empiricFunc}. 

The main idea of the KS test is the analysis of the difference between the given
cumulative distribution function (\cdf) $F$ and the empirical \cdf $F_n$. Since
both theoretical and empirical functions belong to normed space of bounded
functions $\mathbb{B}(\mathbb{R})$ (all values are between 0 and 1), this difference can be measured as
a distance $\|F_n-F\|_{\infty}=\sup_{x \in \mathbb{R}}|F_n(x)-F(x)|$. Figure
\ref{fig:empiricTeorFunc} illustrates the calculated distance between the
empirical \cdf and the theoretical normal \cdf with respect to the given parameters \textit{sample mean}
and \textit{sample variance}. 
%\begin{figure}[h!]
<<empiricFunc, eval=TRUE, echo=FALSE, fig=TRUE, include=FALSE, results=tex>>=
#Take Na as the most likely normaly distributed vector
dat =  Glass.type1$Na
#Return the value of empiric c.d.f on point x
empiric = function(x, sample) {
	sortsample = sort(sample)
	return(length(which(sortsample<=x))/length(sample))
}

#Draws empiric c.d.f.
drawEmpiric = function(sample) {
seq = seq(from = min(sample)-0.2, to = max(sample)+0.2, length.out=1000)
empseq = sapply(seq, function(x) {empiric(x,sample)})
plot(seq,empseq,pch=20,col='blue',main="", 
		cex=0.2, xlab="", ylab = "")
}
seq = seq(from = min(dat)-0.2, to = max(dat)+0.2, length.out=1000)
#draw empirical c.d.f.
drawEmpiric(dat)

@
%\includegraphics[width=\textwidth]{report-empiricFunc}
%\caption{Exemplary empirical \cdf of a data sample.}
%\label{fig:empiricFunc}
%\end{figure}


%\begin{figure}[h!]
<<empiricTeorFunc, eval=TRUE, echo=FALSE, fig=TRUE, include=FALSE, results=tex>>=
<<empiricFunc>>

#empirical values of probabilities
empdat = sapply(seq, function(x) {empiric(x,dat)})

#sample mean and sample covariance
mean = mean(dat)
Cov = var(dat)
empdat = sapply(seq, function(x) {empiric(x,dat)})
theordat = pnorm(seq,mean,Cov)
dif = theordat-empdat

drawDistance = function(sample,mu,sigma, col) {
	#line discretization
	seq = seq(from = min(sample)-0.2, to = max(sample)+0.2, length.out=1000)
		
	#empirical values of probabilities
	empdat = sapply(seq, function(x) {empiric(x,sample)})
	
	#theoretical values of probabilities
	theordat = pnorm(seq,mu,sigma)
	#difference between theoretica and empirical probabilities
	dif = theordat-empdat
	#point of maximum
	pointmax = seq[which(abs(dif)==max(abs(dif)))]
	if(length(pointmax)>1) pointmax=pointmax[1]
	#draw distance between theoretical and empirical c.d.f. 
	points(c(pointmax,pointmax),c(empiric(pointmax,sample),pnorm(pointmax,mu,sigma)), 
			type='l', lty = "dashed", col=col)
	text(pointmax+0.3,(empiric(pointmax,sample)+pnorm(pointmax,mu,sigma))/2,paste("d= ",format(max(abs(dif)), digits=4,nsmall=4)))
}

drawDistance(dat,mean,Cov,"green")

drawTheoreticFunc = function(sample,mu,sigma, col) {
	seq = seq(from = min(sample)-0.2, to = max(sample)+0.2, length.out=1000)
	#draw theoretical normal c.d.f.
	points(seq,pnorm(seq,mu,sigma), type='l', col=col)
}
drawTheoreticFunc(dat,mean,Cov,"red")
legend("bottomright", legend = c("F",expression(F[n]),expression(sup(abs(F[n]-F)))), 
		lty = c("solid","solid","dashed"), lwd = 1, cex=1, col = c(2,4,3)) 
@
%\includegraphics[width=\textwidth]{report-empiricTeorFunc}
%\caption{Exemplary empirical \cdf of a data sample.}
%\label{fig:empiricTeorFunc}
%\end{figure}
\begin{figure}[H]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{report-empiricFunc}
  \vspace{-1cm}
  \caption{}
  \label{fig:empiricFunc}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{report-empiricTeorFunc}
  \vspace{-1cm}
  \caption{}
  \label{fig:empiricTeorFunc}
\end{subfigure}
\caption{Empirical \cdf for Sodium (Na) vector (a) and theoretical normal \cdf with
sample mean and sample variance (b)}
\label{fig:commonFigureKStest}
\end{figure}

The KS test statistics is defined as follows:
\[D_n = \sqrt{n}\cdot \sup_{x \in \mathbb{R}}|F_n(x)-F(x)|.\]
If value $D_i$ is considered for different $1\le i\le n$, then the sample
$\hat{D}_n=(D_1,\dots,D_n)$ is obtained that also complies with some
distribution $\mathbb{D}_n$.
It can be shown that if hypothesis $H_0$ is true, then this distribution does not depend on the \cdf $F$
and therefore can be tabulated. Moreover, Kolmogorov proved that if $n$ is large
enough, the distribution function of $\mathbb{D}_n$ can be approximated by
Kolmogorov-Smirnov distribution function
$H(t)=1-2\sum_{i=1}^{\infty}(-1)^{i-1} \exp^{-2i^2t^2}$, \ie for each positive
value $t$ the probability $P(D_n\le t)\to H(t)$ when $n \to \infty$. More
details can be found in the textbook (DeGroot, Morris, H., Mark, J. 2003).

The KS test uses the decision rule
\[ \delta = 
\left\{
\begin{array}{rcl}
H_0&:& D_n\le c\\
H_1&:& D_n> c
\end{array}
\right.,
\]
where the critical value $c$ depends on the significance level $\alpha$ and
can be calculated from the following equations:
\[\alpha = P(\delta \ne H_0|H_0)=P(D_n>c|H_0)=1-P(D_n\le c|H_0)\approx 1-H(c).\]
As mentioned above the last expression can only be considered appriximately equal when $n$ is
relatively high. Otherwise the table values for $\mathbb{D}_n$ distribution
should be used. Hence, $c\approx H^{-1}(1-\alpha)$.

Although the KS test is commonly used, it has a huge drawback. Namely it
considers only completely defined theoretical \cdf. When testing for normality 
both parameters $\mu$ and $\sigma^2$ have to be predefined. But usually they are
a priori unknown when a sample is going to be tested. Of course it can be
managed by assigning sample mean and sample variance as unknown parameters (and
initial KS test suggests to do that) but they are not always the best choice.
The following example of the Sodium (Na) distribution illustrates this proposition. 

For the univariate Sodium (Na) sample of 70 observations the sample mean
$\bar{\mu}=$\Sexpr{format(mean,digits=4, nsmall=4)} and the sample variance $\bar{\sigma^2}=$
\Sexpr{format(Cov,digits=4, nsmall=4)} are calculated. For these values the
theoretical c.d.f. is determined (Figure \ref{fig:empiricTeorFunc}). Then
the value of KS statistic is calculated: $D_n=\sqrt{n}\cdot
\sup_x|F_n(x)-F(x)|=$\Sexpr{format(sqrt(length(dat))*max(abs(dif)),digits=4, nsmall=4)}.
The critical value for the significance level $\alpha=0.01$ is equal to
$c=H^{-1}(1-\alpha)$=\Sexpr{c=format(Inverse(H,1-0.01),digits=4,nsmall=4)}. 
%To calculate this value the following $R$ functions are used:

<<HFunction, eval=TRUE, echo=FALSE, fig=FALSE, include=FALSE, results=tex, prompt = " ", continue = " ">>=
#Calculates KS distribution function
H= function(t) {
	i=1
	sum=0
	while(abs(f(t,i) - f(t,i+1))>0.000000000001) {
		sum=sum+f(t,i)
		i=i+1
	}
	1-2*sum
}
#Summand of H function
f=function(t,i) {
	(-1)^(i-1)*exp(-2*i^2*t^2)
}
#Returns value c such that H(c)=a 
Inverse = function(H,a) {
	newFunction = function(t) {
		H(t)-a
	}
	#Returns the root of the equation H(x)-a=0
	uniroot(newFunction,c(0.2,4),tol=0.001)$root
}
@
Since $D_n>c$, the null hypothesis is rejected by the KS test. But that does not mean that the sample is not normally distributed. It only means that the hypothesis is rejected for the normal distribution with these specific parameters sample mean and sample variance. 

In order to manage this issue, the KS test is improved by solving the following
optimisation problem \[KS(\mu,\sigma^2)=\sup_{x \in
\mathbb{R}}|F_n(x)-F(x,\mu,\sigma^2)|\to \min.\]
When the new values $\hat{\mu}$ and $\hat{\sigma^2}$ that minimise the value
$KS(\mu,\sigma^2)$ are found, the general KS test is performed with respect to
these values of parameters. For solving this optimisation problem the following
$R$ code is used:

<<KSfunctionoptimisation, eval=TRUE, echo=TRUE>>=
KS= function(param) {
	#discretization of a line segment
	seq = seq(from = min(dat)-0.2, to = max(dat)+0.2, length.out=1000)
	#values of empirical c.d.f.
	empdat = sapply(seq, function(x) {empiric(x,dat)})
	#values of theoretical c.d.f.
	theordat = pnorm(seq,param[1],abs(param[2]))
	#difference between the values
	dif=theordat-empdat
	absdiff=abs(dif)
	max(absdiff)
}
#optim is a predifined R function in stats package
#defalut method of optimisation is Nelder and Mead (1965)
KSoptim = optim(c(mean,Cov),KS)
KSoptim$par
KSoptim$value
@
These new parameters
$\hat{\mu}=$\Sexpr{format(KSoptim$par[1],digits=4,nsmall=4)} and
$\hat{\sigma}^2=$\Sexpr{format(KSoptim$par[2],digits=4,nsmall=4)} are taken as
parameters of a new theoretical normal \cdf and a new distance is calculated
(Figure \ref{fig:improvedKS}). 
\begin{figure}[H]
<<improvedKS, eval=TRUE, echo=FALSE, fig=TRUE, include=FALSE, results=tex>>=
#draw empirical c.d.f.
drawEmpiric(dat)

#new optimised mu and sigma
mean1 = KSoptim$par[1]
Cov1 = KSoptim$par[2]


#draw old theoretical normal c.d.f.
drawTheoreticFunc(dat,mean,Cov,"red")

#draw improved one
drawTheoreticFunc(dat,mean1,Cov1,"black")

#draw the distance between the first one and empirical c.d.f.
drawDistance(dat,mean,Cov,"green")

#draw the distance between the second one and empirical c.d.f.
drawDistance(dat,mean1,Cov1,"cyan")

legend("bottomright", legend = c("F",expression(F[n]),expression(sup(abs(F[n]-F))),expression(F[new]),expression(sup(abs(F[n]-F[new])))), 
		lty = c("solid","solid","dashed","solid","dashed"), lwd = 1, cex=1, col = c(2,4,3,1,5)) 
@
\includegraphics[width=\textwidth]{report-improvedKS}
\caption{Normal \cdf with optimised parameters in comparison to the old \cdf
with sample mean and sample variance.}
\label{fig:improvedKS}
\end{figure}

The new value of the KS statistics is $D_n=\sqrt{n}\sup_{x \in
\mathbb{R}}|F_n(x)-F(x,\hat{\mu},\hat{\sigma}^2)|=$\Sexpr{format(sqrt(length(dat))*KSoptim$value,digits=4,nsmall=4)}
which is smaller than the critical value $c=$\Sexpr{c}. Therefore, the null
hypothesis $H_0$ cannot further be rejected by KS-test. For all further samples, the improved KS
test is used. 
%It was implemented in $R$ and has the following code.

<<fullImprovedKStest, eval=FALSE, echo=FALSE, prompt = " ", continue = " ">>=
# Performs a Kolmogorov-Smirnov goodness of fit test on the given data
# for the assumption of a normal distribution.
# Returns true if the null hypothesis (sample drawn from a
# normal distributed population) is rejected, false otherwise.
# The parameters are optimised.
# The significance level is determined by sig.
KSimpr.test.norm = function(data, sig) {
	critical_value = Inverse(H,1-sig)
	#Sample mean and sample variance calculation
	mu = mean(data)
	sigma = var(data)
	#KS function that has to be minimized.
	KS= function(param) {
		#discretization of a line segment
		seq = seq(from = min(data)-0.2, 
				to = max(data)+0.2, length.out=1000)
		#values of empirical c.d.f.
		empdat = sapply(seq, function(x) {empiric(x,data)})
		#values of theoretical c.d.f.
		theordat = pnorm(seq,param[1],abs(param[2]))
		#difference between the values
		dif=theordat-empdat
		absdiff=abs(dif)
		max(absdiff)
		}	
	#optim is a predifined R function in stats package
	#defalut method of optimisation is Nelder and Mead (1965)
	KSoptim = optim(c(mu,sigma),KS)
	teststat = sqrt(length(data))*KSoptim$value
	p_value = 1-H(teststat)
	
	print(teststat > critical_value)
	
	return(list( mean = mu,
				var = abs(sigma),
				mu_opt = KSoptim$par[1],
				sigma_opt = abs(KSoptim$par[2]),
				teststat = teststat,
				critical_value = critical_value,
				p_value = p_value,
				rejected = teststat > critical_value))
}

KSimpr.test.norm(dat,0.01)
@

The approximation problem with the $H$ function remains in the improved KS test
as well. Therefore this approximation is only applicable when 
the sample size is relatively high. Otherwise, the tabular values for
$\mathbb{D}_n$ distribution should be used. 



\subsection{Box-Cox-transformation}\label{sec:boxcox-theory}

If data are not normally distributed, it can still be transformed to fit to a normal distribtion in some cases. One possibility is the Box-Cox-transformation. It is a family of parameterised power tranformations:
\[
  \phantom{\quad\mbox{for}\ x > 0}
   x^{(\lambda)} =
   \left\{ 
    \begin{array}{cl}
%\vspace{12pt}
                 \frac{x^\lambda - 1}{\lambda} & \lambda \neq 0\\
                 \ln(x) & \lambda = 0
    \end{array}
   \right.
   \quad\mbox{for}\ x > 0
\]
The optimal parameter for specific observations $x_1, \dots, x_n$ can be determined by a maximum-likelihood estimation, maximising the log likelihood
\[
\begin{array}{l}
\vspace{12pt}
   l(\lambda) = - \frac{n}{2}\ln\left[\frac{1}{n}\sum_{j=1}^{n}(x_j^{(\lambda)} - \overline{x^{(\lambda)}})^2\right] + (\lambda - 1) \sum_{j=1}^{n}\ln(x_j)\\
   \mbox{with}\ \overline{x^{(\lambda)}} = \frac{1}{n} \sum_{j=1}^{n}x_j^{(\lambda)}
\end{array}
\]
However, a Box-Cox-transformation does not ensure that the data is normally distributed thereafter. One reason that a sample cannot be properly transformed could be that it is not unimodal. Histograms and QQ-plots of a sample from a unimodal distribution are depicted in figure \ref{fig:transformationUnimodal}. Data that is generated from a Weibull distribution can be transformed to approximately normally distributed values quite well as can be recognised by the histogram and the QQ-plot. In contrast, it is not possible to properly transform a sample that is combined from two different distributions (here with different scale parameters of the Weibull distribution) as shown in figure \ref{fig:transformationBimodal}. By the combination of two samples with different mean values a bimodal sample emerges preventing the underlying data to be transformed to a unimodal sample (namely a normally distributed sample) by a simple function. Furthermore, noisy data is not suited for Box-Cox-transformation either because the Box-Cox-function is applied on the whole sample (and not only the "noisy parts").
\newpage
\phantom{.}
\vfill

\begin{figure}[h!]
<<transformationUnimodal, eval=TRUE, echo=FALSE, term=FALSE, fig=TRUE, include=FALSE>>=
par(mfrow=c(2,2))
set.seed(549)
x = rweibull(500, 1)
plot(hist(x, plot=FALSE), main="Histogram of a Weibull(1, 1)\n sample of size 500", xlab="Weibull(1, 1)")
lambda = powerTransform(x)
x.transformed = boxcox(x, lambda$lambda)
plot(hist(x.transformed, plot=FALSE), main="Histogram of the Box-Cox-\n transformed Weibull(1, 1) sample", xlab="transformed Weibull(1, 1)")
plot(qqnorm(x, plot=FALSE), main="Normal Q-Q Plot", xlab="Weibull(1, 1)", ylab="Sample Quantiles")
qqline(x,lwd=2,col="red")
plot(qqnorm(x.transformed, plot=FALSE), main="Normal Q-Q Plot", xlab="transformed Weibull(1, 1)", ylab="Sample Quantiles")
qqline(x.transformed,lwd=2,col="red")
par(mfrow=c(1,1))
@
\includegraphics[width=\textwidth]{report-transformationUnimodal}
\caption{Histograms and QQ-plots of a Weibull(1, 1) simulated sample of size 500 and of the Box-Cox-transformed data}
\label{fig:transformationUnimodal}
\end{figure}

\vfill

\newpage
\phantom{.}
\vfill

\begin{figure}[h!]
<<transformationBimodal, eval=TRUE, echo=FALSE, term=FALSE, fig=TRUE, include=FALSE>>=
par(mfrow=c(2,2))
set.seed(518161878)
x.1 = rweibull(250, 5, 1)
x.2 = rweibull(250, 5, 4)
x.bimodal = c(x.1, x.2)
plot(hist(x.bimodal, plot=FALSE), main="Histogram of a Weibull(5, 1) /\n Weibull(5, 4) mixed sample", xlab="Weibull(5, 1), Weibull(5, 4)")
lambda = powerTransform(x.bimodal)
x.bimodal.transformed = boxcox(x.bimodal, lambda$lambda)
plot(hist(x.bimodal.transformed, plot=FALSE), main="Histogram of the Box-Cox-\n transfomed mixed sample", xlab="transformed Weibull(5, 1), Weibull(5, 4)")
plot(qqnorm(x.bimodal, plot=FALSE), main="Normal Q-Q Plot", xlab="Weibull(5, 1), Weibull(5, 4)", ylab="Sample Quantiles")
qqline(x.bimodal,lwd=2,col="red")
plot(qqnorm(x.bimodal.transformed, plot=FALSE), main="Normal Q-Q Plot", xlab="transformed Weibull(5, 1), Weibull(5, 4)", ylab="Sample Quantiles")
qqline(x.bimodal.transformed,lwd=2,col="red")
par(mfrow=c(1,1))
@
\includegraphics[width=\textwidth]{report-transformationBimodal}
\caption{Histograms and QQ-plots of a mixed sample composed of a Weibull(5, 1) simulated sample and a Weibull(5, 4) simulated sample (each of size 250) and of the Box-Cox-transformed data}
\label{fig:transformationBimodal}
\end{figure}

\vfill


\newpage
\section{Testing the data sample for normality}\label{sec:testing}

\subsection{Testing original data}


The test methods for normality that were introduced in section \ref{sec:methods} are now applied on the glass data set. For each method, the whole sample is tested first. Since it can be assumed that the different glass types are distinct in terms of the underlying distribution of variable values, the tests are conducted on the individual types as well (where applicable). It appears reasonable to skip particular variables whose values predominantly consist of zeros (more than 50\,\% of the observations, see table \ref{tab:zeros}) because this indicates that those variables are not normally distributed anyway (moreover, this can lead to complications for some methods).

\begin{table}[h!]
\centering
\begin{tabular}{|cl|}
\hline
type & skipped variables\\
\hline
1 & Ba\\
2 & Ba\\
3 & Ba, Fe\\
5 & Ba, Fe\\
6 & K, Ba, Fe\\
7 & Mg\\
\hline
\end{tabular}
\caption{Skipped variables for the particular glass types due to too many zero values}
\label{tab:zeros}
\end{table}

\subsubsection{Q-Q-plot}\label{sec:qq-original}
For the graphical analysis the quantiles for all elements in the glass-dataset where plotted for the whole dataset and for each glass type separately. As depicted in Figure \ref{fig:QQfull}, the QQ-Plots for the five variables recursive index (RI), Sodium (Na), Aluminium (Al), Silicon (Si), and Calcium (Ca) do suggest that linear relationships could be assumed. The other four variables Magnesium (Mg), Barium (Ba), Potassium (K), and Iron (Fe) clearly are not linearly related to the hypothetical quantiles.

\begin{figure}[h!]
\centering
<<QQfull, eval=TRUE, echo=FALSE, term=FALSE, fig=TRUE, include=FALSE>>=
Glass.full <- Glass[,-10]
# QQ-Plots for each Element in the full Dataset 
layout(matrix(1:9, ncol=3,nrow=3)) 
sapply(colnames(Glass.full), function(x) {
			qqnorm(Glass.full[[x]], main = x,pch=19,
					cex.lab=1,cex.main=1,ylab=" ")
			qqline(Glass.full[[x]],lwd=1,col="red")
		})
@
\includegraphics[width=\textwidth]{report-QQfull}
\caption{Exemplary QQ-Plots from the full data sample}
\label{fig:QQfull}
\end{figure}


\begin{figure}[h!]
\centering
<<QQsamples, eval=TRUE, echo=FALSE, term=FALSE, fig=TRUE, include=FALSE>>=
par(mfrow=c(2,2))
qqnorm(Glass.type1s[["Si"]], main ="Si in Glass Type 1",pch=19,
       cex.lab=1)
qqline(Glass.type1s[["Si"]],lwd=2,col="red")
qqnorm(Glass.type1s[["K"]], main ="K in Glass Type 1",pch=19,
       cex.lab=1)
qqline(Glass.type1s[["K"]],lwd=2,col="red")
qqnorm(Glass.type2s[["Ca"]], main ="Ca in Glass Type 2",pch=19,
       cex.lab=1)
qqline(Glass.type2s[["Ca"]],lwd=2,col="red")
qqnorm(Glass.type2s[["K"]], main ="K in Glass Type 2",pch=19,
       cex.lab=1)
qqline(Glass.type2s[["K"]],lwd=2,col="red")
@
\includegraphics[width=\textwidth]{report-QQsamples}
\caption{Exemplary QQ-Plots from Glass Type 1 where a graphical inspection does not suggest a linear relationship}
\label{fig:QQsamples}
\end{figure}

The graphical comparison of the subdatasets shows that for most of the cases there seems to be no linear relationship between the theoretical normally distributed quantiles and the observed values. For some cases in the subdatasets, however, based on a graphical inspection a linear relationship can be assumed given some outliers. As shown in Figure \ref{fig:QQsampleslin1} Magnesium (Mg), Aluminium(Al), and Silicon (Si) in glass type 1 and Ca in glass type 7 show approximately linear relationships with just a few outliers. For some observations (e.g. Na in glass type 5 and RI in glass type 6 in Figure \ref{fig:QQsampleslin2}) a linear relationship seems to be plausible, however, the total number of observations in these cases is considered too small to make conclusions about a hypothetical linear relationship (glass type 5: $n$ = 13; glass type 6: $n$ = 9).

\begin{figure}[h!]
\centering
<<QQsampleslin1, eval=TRUE, echo=FALSE, term=FALSE, fig=TRUE, include=FALSE>>=
par(mfrow=c(2,2))
qqnorm(Glass.type1s[["Mg"]], main ="Mg in Glass Type 1",pch=19,
       cex.lab=1)
qqline(Glass.type1s[["Mg"]],lwd=2,col="red")
qqnorm(Glass.type2s[["Al"]], main ="Al in Glass Type 1",pch=19,
       cex.lab=1)
qqline(Glass.type2s[["Al"]],lwd=2,col="red")
qqnorm(Glass.type7s[["Si"]], main ="Si in Glass Type 1",pch=19,
       cex.lab=1)
qqline(Glass.type7s[["Si"]],lwd=2,col="red")
qqnorm(Glass.type7s[["Ca"]], main ="Ca in Glass Type 7",pch=19,
       cex.lab=1)
qqline(Glass.type7s[["Ca"]],lwd=2,col="red")
@
\includegraphics[width=\textwidth]{report-QQsampleslin1}
\caption{QQ-Plots of the cases where a linear relationship seems plausible}
\label{fig:QQsampleslin1}
\end{figure}

%\begin{figure}[h!]
%\centering
<<QQsampleslin2a, eval=TRUE, echo=FALSE, term=FALSE, fig=TRUE, include=FALSE>>=
qqnorm(Glass.type5s[["Na"]], main ="Na in Glass Type 5",pch=19,
       cex.lab=1)
qqline(Glass.type5s[["Na"]],lwd=2,col="red")
@
%\includegraphics[width=\textwidth]{report-QQsampleslin2}
%\caption{QQ-Plots of possible linear relationships with very small sample sizes}
%\label{fig:QQsampleslin2}
%\end{figure}

%\begin{figure}[h!]
%\centering
<<QQsampleslin2b, eval=TRUE, echo=FALSE, term=FALSE, fig=TRUE, include=FALSE>>=
qqnorm(Glass.type6s[["RI"]], main ="RI in Glass Type 6",pch=19,
       cex.lab=1)
qqline(Glass.type6s[["RI"]],lwd=2,col="red")
@
%\includegraphics[width=\textwidth]{report-QQsampleslin2}
%\caption{QQ-Plots of possible linear relationships with very small sample sizes}
%\label{fig:QQsampleslin2}
%\end{figure}

\begin{figure}[h!]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{report-QQsampleslin2a}
  \vspace{-1cm}
  \caption{}
  \label{fig:QQsampleslin2a}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{report-QQsampleslin2b}
  \vspace{-1cm}
  \caption{}
  \label{fig:QQsampleslin2b}
\end{subfigure}
\caption{QQ-Plots of possible linear relationships with very small sample sizes}
\label{fig:QQsampleslin2}
\end{figure}

\subsubsection{Shapiro-Wilk test}\label{sec:shapiro-original}
Performing the Shapiro-Wilk test on the complete dataset at hand, according to the p-values the null hypothesis (the data the sample is taken from is normally distributed) is rejected for all the elements.
Testing the different glass types separately, the p-values of majority of the variables are still very small. Consequently, the null hypotheses can be rejected for almost all separate cases. For a total 16 cases (over all 6 glass types) the test returned a p-value above or equal to the set alpha-level of 1\, \% (Tables \ref{tab:testrestype1SW} to \ref{tab:testrestype7SW}). However, the majority of the cases for which the null hypothesis holds stems from the three subdatasets containing significantly less observations (Tables \ref{tab:testrestype3SW} to \ref{tab:testrestype6SW}). For glass type 1 $H_0$ is rejected for all the variables (see Table \ref{tab:testrestype1SW}). For glass type 2 it only holds for Aluminium (Al) and for glass type 7 only for Aluminium (Al) and Barium (Ba) (see Tables \ref{tab:testrestype2SW} and \ref{tab:testrestype7SW}).
More than 80\, \% of the total rejections in the 6 tests (13 out of 16) stem from the three smaller subdatasets glass type 3, glass type 5, and glass type 6. These results raise the question if the low number of rejections is due to the smaller sample size in these cases. Looking only at the glass types 1, 2, and 7, a normal distribution can be rejected for most of the cases.


<<calcSWresults, echo=FALSE, term=TRUE>>=
sig=0.01
shapresult.full <- apply(Glass[,-10], 2, shapiro.test)
shapresult.type1 <- apply(Glass.type1s, 2, shapiro.test)
shapresult.type2 <- apply(Glass.type2s, 2, shapiro.test)
shapresult.type3 <- apply(Glass.type3s, 2, shapiro.test)
shapresult.type5 <- apply(Glass.type5s, 2, shapiro.test)
shapresult.type6 <- apply(Glass.type6s, 2, shapiro.test)
shapresult.type7 <- apply(Glass.type7s, 2, shapiro.test)
@

\begin{table}[h!]
\centering
<<testresFullSW, echo=FALSE, term=TRUE, results=tex>>=
cat("\\begin{tabular}{|cccccc|} ")
cat("\\hline ")
cat(paste("variable & test statistic & sig. level & critical value & p-value & rejected\\\\ ", "\n", sep=""))
cat("\\hline ")
tr=shapresult.full

for(i in 1:(length(tr))) {
  varname = names(tr[i])
  pvalue = tr[[varname]][["p.value"]]
  cat(paste(
    varname,
    round(tr[[varname]][["statistic"]], 2),
    sig,
    "NA",
    ifelse(pvalue < 1e-15, "< 1.0e-15", pvalue),
    ifelse(pvalue<=sig, "yes", "no"),
           sep=" & "), "\\\\ ", "\n", sep="")
}

cat("\\hline ")
cat(paste("\\end{tabular}", "\n", sep=""))
@
\caption{Test results of the Shapiro-Wilk test on the whole data sample}
\label{tab:testresFullSW}
\end{table}

\begin{table}[h!]
\centering
<<testrestype1SW, echo=FALSE, term=TRUE, results=tex>>=
cat("\\begin{tabular}{|cccccc|} ")
cat("\\hline ")
cat(paste("variable & test statistic & sig. level & critical value & p-value & rejected\\\\ ", "\n", sep=""))
cat("\\hline ")
tr=shapresult.type1

for(i in 1:(length(tr))) {
  varname = names(tr[i])
  pvalue = tr[[varname]][["p.value"]]
  cat(paste(
    varname,
    round(tr[[varname]][["statistic"]], 2),
    sig,
    "NA",
    ifelse(pvalue < 1e-15, "< 1.0e-15", pvalue),
    ifelse(pvalue<=sig, "yes", "no"),
           sep=" & "), "\\\\ ", "\n", sep="")
}

cat("\\hline ")
cat(paste("\\end{tabular}", "\n", sep=""))
@
\caption{Test results of the Shapiro-Wilk test on type 1 glass}
\label{tab:testrestype1SW}
\end{table}

\begin{table}[h!]
\centering
<<testrestype2SW, echo=FALSE, term=TRUE, results=tex>>=
cat("\\begin{tabular}{|cccccc|} ")
cat("\\hline ")
cat(paste("variable & test statistic & sig. level & critical value & p-value & rejected\\\\ ", "\n", sep=""))
cat("\\hline ")
tr=shapresult.type2

for(i in 1:(length(tr))) {
  varname = names(tr[i])
  pvalue = tr[[varname]][["p.value"]]
  cat(paste(
    varname,
    round(tr[[varname]][["statistic"]], 2),
    sig,
    "NA",
    ifelse(pvalue < 1e-15, "< 1.0e-15", pvalue),
    ifelse(pvalue<=sig, "yes", "no"),
           sep=" & "), "\\\\ ", "\n", sep="")
}

cat("\\hline ")
cat(paste("\\end{tabular}", "\n", sep=""))
@
\caption{Test results of the Shapiro-Wilk test on type 2 glass}
\label{tab:testrestype2SW}
\end{table}

\begin{table}[h!]
\centering
<<testrestype3SW, echo=FALSE, term=TRUE, results=tex>>=
cat("\\begin{tabular}{|cccccc|} ")
cat("\\hline ")
cat(paste("variable & test statistic & sig. level & critical value & p-value & rejected\\\\ ", "\n", sep=""))
cat("\\hline ")
tr=shapresult.type3

for(i in 1:(length(tr))) {
  varname = names(tr[i])
  pvalue = tr[[varname]][["p.value"]]
  cat(paste(
    varname,
    round(tr[[varname]][["statistic"]], 2),
    sig,
    "NA",
    ifelse(pvalue < 1e-15, "< 1.0e-15", pvalue),
    ifelse(pvalue<=sig, "yes", "no"),
           sep=" & "), "\\\\ ", "\n", sep="")
}

cat("\\hline ")
cat(paste("\\end{tabular}", "\n", sep=""))
@
\caption{Test results of the Shapiro-Wilk test on type 3 glass}
\label{tab:testrestype3SW}
\end{table}

\begin{table}[h!]
\centering
<<testrestype5SW, echo=FALSE, term=TRUE, results=tex>>=
cat("\\begin{tabular}{|cccccc|} ")
cat("\\hline ")
cat(paste("variable & test statistic & sig. level & critical value & p-value & rejected\\\\ ", "\n", sep=""))
cat("\\hline ")
tr=shapresult.type5

for(i in 1:(length(tr))) {
  varname = names(tr[i])
  pvalue = tr[[varname]][["p.value"]]
  cat(paste(
    varname,
    round(tr[[varname]][["statistic"]], 2),
    sig,
    "NA",
    ifelse(pvalue < 1e-15, "< 1.0e-15", pvalue),
    ifelse(pvalue<=sig, "yes", "no"),
           sep=" & "), "\\\\ ", "\n", sep="")
}

cat("\\hline ")
cat(paste("\\end{tabular}", "\n", sep=""))
@
\caption{Test results of the Shapiro-Wilk test on type 5 glass}
\label{tab:testrestype5SW}
\end{table}

\begin{table}[h!]
\centering
<<testrestype6SW, echo=FALSE, term=TRUE, results=tex>>=
cat("\\begin{tabular}{|cccccc|} ")
cat("\\hline ")
cat(paste("variable & test statistic & sig. level & critical value & p-value & rejected\\\\ ", "\n", sep=""))
cat("\\hline ")
tr=shapresult.type6

for(i in 1:(length(tr))) {
  varname = names(tr[i])
  pvalue = tr[[varname]][["p.value"]]
  cat(paste(
    varname,
    round(tr[[varname]][["statistic"]], 2),
    sig,
    "NA",
    ifelse(pvalue < 1e-15, "< 1.0e-15", pvalue),
    ifelse(pvalue<=sig, "yes", "no"),
           sep=" & "), "\\\\ ", "\n", sep="")
}

cat("\\hline ")
cat(paste("\\end{tabular}", "\n", sep=""))
@
\caption{Test results of the Shapiro-Wilk test on type 6 glass}
\label{tab:testrestype6SW}
\end{table}

\begin{table}[h!]
\centering
<<testrestype7SW, echo=FALSE, term=TRUE, results=tex>>=
cat("\\begin{tabular}{|cccccc|} ")
cat("\\hline ")
cat(paste("variable & test statistic & sig. level & critical value & p-value & rejected\\\\ ", "\n", sep=""))
cat("\\hline ")
tr=shapresult.type7

for(i in 1:(length(tr))) {
  varname = names(tr[i])
  pvalue = tr[[varname]][["p.value"]]
  cat(paste(
    varname,
    round(tr[[varname]][["statistic"]], 2),
    sig,
    "NA",
    ifelse(pvalue < 1e-15, "< 1.0e-15", pvalue),
    ifelse(pvalue<=sig, "yes", "no"),
           sep=" & "), "\\\\ ", "\n", sep="")
}

cat("\\hline ")
cat(paste("\\end{tabular}", "\n", sep=""))
@
\caption{Test results of the Shapiro-Wilk test on type 7 glass}
\label{tab:testrestype7SW}
\end{table}


\subsubsection{Pearson's chi-squared test}
<<calculatChisq, echo=FALSE>>=
testresult.full = apply(Glass[, -10], 2, chisq.test.norm, initial_breaks=10, min=5, sig=0.01)
testresult.type1 = apply(Glass.type1[, c(-8,-10)], 2, chisq.test.norm, initial_breaks=10, min=5, sig=0.01)
testresult.type2 = apply(Glass.type2[, c(-8,-10)], 2, chisq.test.norm, initial_breaks=10, min=5, sig=0.01)
testresult.type7 = apply(Glass.type7[, c(-3,-9)], 2, chisq.test.norm, initial_breaks=10, min=5, sig=0.01)

testresult.full.30 = apply(Glass[, -10], 2, chisq.test.norm, initial_breaks=30, min=5, sig=0.01)
testresult.type1.30 = apply(Glass.type1[, c(-8,-10)], 2, chisq.test.norm, initial_breaks=30, min=5, sig=0.01)
testresult.type2.30 = apply(Glass.type2[, c(-8,-10)], 2, chisq.test.norm, initial_breaks=30, min=5, sig=0.01)
testresult.type7.30 = apply(Glass.type7[, c(-3,-9)], 2, chisq.test.norm, initial_breaks=30, min=5, sig=0.01)
@

As mentioned in section \ref{sec:chisq-theoretical}, Pearson's chi-squared test is not suited for rather small sample sizes because of the approximation via the chi-squared distribution. Concerning the given data, the samples of type 3 glass (\Sexpr{length(Glass.type3$RI)} observations), type 5 glass (\Sexpr{length(Glass.type5$RI)} observations) and type 6 glass (\Sexpr{length(Glass.type6$RI)} observations) are not large enough to ensure a viable test result. Hence, the data belonging to those types will not be considered for separate tests. However, it will remain in the overall data sample of all types. The minimum size of observations in each class is set to five and the number of initial classes (\ie number of classes before unifying) will be ten. The first tests are conducted on the whole data set for each variable. The results are shown in table \ref{tab:chi-full}. For two variables, it is not possible to determine a test result with the given parameters: The observations of the variables Potassium (K) and Barium (Ba) are divided only into three classes respectively after the unification of classes in order to fulfill the requirement of minimum class size. Since one degree of freedom is subtracted always and two degrees of freedom are subtracted for the estimation of the mean value and the standard deviation, zero degrees of freedom remain and so the critical value cannot be calculated. For each of the other variables, the hypothesis of normality is clearly rejected for the given significance level.

\begin{table}[h!]
\centering
<<testresFull, echo=FALSE, term=TRUE, results=tex>>=
cat("\\begin{tabular}{|cccccc|} ")
cat("\\hline ")
cat(paste("variable & test statistic & sig. level & critical value & p-value & rejected\\\\ ", "\n", sep=""))
cat("\\hline ")
tr=testresult.full

for(i in 1:(length(tr))) {
  cat(paste(
    names(tr[i]),
    round(tr[i][[1]]$teststat, 2),
    0.01,
    round(tr[i][[1]]$critical_value, 2),
    ifelse(tr[i][[1]]$p_value < 1e-15, "< 1.0e-15", tr[i][[1]]$p_value),
    ifelse(tr[i][[1]]$rejected, "yes", "no"),
           sep=" & "), "\\\\ ", "\n", sep="")
}

cat("\\hline ")
cat(paste("\\end{tabular}", "\n", sep=""))
@
\caption{Test results of the chi-squared test on the whole data sample with ten initial classes}
\label{tab:chi-full}
\end{table}

The results for type 1 glass (table \ref{tab:chi-type1}) are slightly different; in this case, the results can be determined for each variable (except for Barium (Ba), which has been dropped beforehand) and the hypothesis of normality is rejected for each variable but Sodium (Na). The p-value for Sodium is comparably high amounting to approximately \Sexpr{round(testresult.type1$Na$p_value, 2)}. It is well recognisable that the observed class frequencies for Sodium fluctuate around the expected class frequencies under the hypothesis of a normal distribution with the according parameters (table \ref{tab:testresChisqFreqNaType1}). The good compliance of empirical and hypothetical data for this variable is illustrated in figure \ref{fig:chisqType1Na}. In general, the p-values for this part of the sample are higher than those for the whole sample.

\begin{table}[h!]
\centering
<<testresType1, echo=FALSE, term=TRUE, results=tex>>=
cat("\\begin{tabular}{|cccccc|} ")
cat("\\hline ")
cat(paste("variable & test statistic & sig. level & critical value & p-value & rejected\\\\ ", "\n", sep=""))
cat("\\hline ")
tr=testresult.type1

for(i in 1:(length(tr))) {
  cat(paste(
    names(tr[i]),
    round(tr[i][[1]]$teststat, 2),
    0.01,
    round(tr[i][[1]]$critical_value, 2),
    ifelse(tr[i][[1]]$p_value < 1e-15, "< 1.0e-15", tr[i][[1]]$p_value),
    ifelse(tr[i][[1]]$rejected, "yes", "no"),
           sep=" & "), "\\\\ ", "\n", sep="")
}

cat("\\hline ")
cat(paste("\\end{tabular}", "\n", sep=""))
@
\caption{Test results of the chi-squared test on type 1 glass with ten initial classes}
\label{tab:chi-type1}
\end{table}

\begin{table}[h!]
\centering
<<testresChisqFreqNaType1, echo=FALSE, term=TRUE, results=tex>>=
cat("\\begin{tabular}{|c|cc|} ")
cat("\\hline ")
cat(paste("class & \\multicolumn{2}{c|}{frequencies}\\\\ ", "\n", sep=""))
cat(paste("(interval) & observed & expected\\\\ ", "\n", sep=""))
cat("\\hline ")
tr=testresult.type1$Na

for(i in 1:(length(tr$hist$counts))) {
  cat(paste(
    paste0("]", tr$hist$breaks[i], ", ", tr$hist$breaks[i+1], "]"),
    tr$hist$counts[i],
    round(tr$expected_frequencies[i], 2),
           sep=" & "), "\\\\ ", "\n", sep="")
}

cat("\\hline ")
cat(paste("\\end{tabular}", "\n", sep=""))
@
\caption{Observed end expected frequencies of items in the classes for the variable Sodium of type 1 glass}
\label{tab:testresChisqFreqNaType1}
\end{table}

\begin{figure}[h!]
\centering
<<chisqType1Na, eval=TRUE, echo=FALSE, term=FALSE, fig=TRUE, include=FALSE>>=
testres = chisq.test.norm(Glass.type1$Na, 10, 5, 0.05)
hist=testres$hist
hist.exp=histFromExpectedFreqs(testres)
plot(hist, col=rgb(1, 0, 0,0.5), main="Histogram of observed and expected densities", xlab="amount of Sodium in weight percent")
plot(hist.exp, col=rgb(0, 0, 1,0.5), add=TRUE)
@
\includegraphics[width=0.8\textwidth]{report-chisqType1Na}
\caption{Histogram of observed densities (red) and expected densities (blue) within the classes for the variable Sodium of type 1 glass}
\label{fig:chisqType1Na}
\end{figure}

The test results for observations of type 2 glass are summarised in table \ref{tab:chi-type2}. The null hypothesis is rejected for all variables except for Aluminium (Al) and Silicon (Si). The p-values for these variables are however rather small (approximately \Sexpr{round(testresult.type2$Al$p_value, 2)} and \Sexpr{round(testresult.type2$Si$p_value, 2)}).

\begin{table}[h]
\centering
<<testresType2, echo=FALSE, term=TRUE, results=tex>>=
cat("\\begin{tabular}{|cccccc|} ")
cat("\\hline ")
cat(paste("variable & test statistic & sig. level & critical value & p-value & rejected\\\\ ", "\n", sep=""))
cat("\\hline ")
tr=testresult.type2

for(i in 1:(length(tr))) {
  cat(paste(
    names(tr[i]),
    round(tr[i][[1]]$teststat, 2),
    0.01,
    round(tr[i][[1]]$critical_value, 2),
    ifelse(tr[i][[1]]$p_value < 1e-15, "< 1.0e-15", tr[i][[1]]$p_value),
    ifelse(tr[i][[1]]$rejected, "yes", "no"),
           sep=" & "), "\\\\ ", "\n", sep="")
}

cat("\\hline ")
cat(paste("\\end{tabular}", "\n", sep=""))
@
\caption{Test results of the chi-squared test on type 2 glass with ten initial classes}
\label{tab:chi-type2}
\end{table}

For the observations of type 7 glass, test results (table \ref{tab:chi-type7}) are only available for the variable Aluminium (Al). Due to the small sample size of \Sexpr{length(Glass.type7$RI)} observations, most of the initial classes are joined so that no degree of freedom remains for the chi-squared distribution function. The hypothesis of normality is not rejected for the data of Aluminium.

\begin{table}[h!]
\centering
<<testresType7, echo=FALSE, term=TRUE, results=tex>>=
cat("\\begin{tabular}{|cccccc|} ")
cat("\\hline ")
cat(paste("variable & test statistic & sig. level & critical value & p-value & rejected\\\\ ", "\n", sep=""))
cat("\\hline ")
tr=testresult.type7

for(i in 1:(length(tr))) {
  cat(paste(
    names(tr[i]),
    round(tr[i][[1]]$teststat, 2),
    0.01,
    round(tr[i][[1]]$critical_value, 2),
    ifelse(tr[i][[1]]$p_value < 1e-15, "< 1.0e-15", tr[i][[1]]$p_value),
    ifelse(tr[i][[1]]$rejected, "yes", "no"),
           sep=" & "), "\\\\ ", "\n", sep="")
}

cat("\\hline ")
cat(paste("\\end{tabular}", "\n", sep=""))
@
\caption{Test results of the chi-squared test on type 7 glass with ten initial classes}
\label{tab:chi-type7}
\end{table}

As mentioned in section \ref{sec:chisq-theoretical}, Pearson's chi-squared test is inconsistent when the number or bounds of classes are changed. This inconsistency can also be observed with the present data set. The test have also been conducted with 30 initial classes each (see tables \ref{tab:chi-full-30} to \ref{tab:chi-type7-30} in the appendix) with partly different results. Whereas with ten initial classes, there are not enough classes left for most of the variables of type 7 glass to be tested, the data is divided in a sufficient number of classes when using 30 initial classes. Above all, the null hypothesis is not rejected for Aluminium (Al) of type 2 glass with ten initial classes but it is rejected with 30 initial classes while the opposite is true for Sodium (Na). In general, the p-values can alternate much with different classes; so the rather high p-value for Sodium of type 1 glass ($\sim$ \Sexpr{round(testresult.type1$Na$p_value, 2)}) with ten initial classes decreases to approximately \Sexpr{round(testresult.type1.30$Na$p_value, 2)} with 30 initial classes. On the contrary, the p-value for Aluminium of type 7 glass ($\sim$ \Sexpr{round(testresult.type7$Al$p_value, 2)}) increases to approximately \Sexpr{round(testresult.type7.30$Al$p_value, 2)}. These different impacts on the test results are due to two opposing effects: First, with more classes there are more degrees of freedom for the chi-squared distribution and thus the critical value increases. Second, the test statistic tends to increase as well because the observations have to fit to smaller classes more precisely; or in other words, observations may be distorted (relatively to the hypothetical expectations) within a large class so that differences between empirical and hypothetical data do not raise the test statistic as much as the same observations would if they were divided into smaller classes (making the distortion "measurable").

\subsubsection{Kolmogorov-Smirnov test}

<<calculatKS, eval=FALSE ,echo=FALSE>>=
testresultKS.full = apply(Glass[, -10], 2, KSimpr.test.norm, sig=0.01)
testresultKS.type1 = apply(Glass.type1[, c(-8,-10)], 2, KSimpr.test.norm, sig=0.01)
testresultKS.type2 = apply(Glass.type2[, c(-8,-10)], 2, KSimpr.test.norm, sig=0.01)
testresultKS.type7 = apply(Glass.type7[, c(-3,-9)], 2, KSimpr.test.norm, sig=0.01)
@


First and foremost, the Kolmogorov-Smirnov test is conducted on the whole dataset for each variable. As depicted in table \ref{tab:KS-full}, the results indicate that only for Magnesium (Mg), Potassium (K), Barium (Ba), and Iron (Fe) $H_0$ can be rejected. For all other variables the hypothesis that the data the sample stems from is normally distributed cannot be rejected.

Performing the test on the subclasses type 1 glass, type 2 glass, and type 7 glass for each variable, the results further support the notion that data in the different subclasses is normally distributed.
While for type 1 glass $H_0$ can only be rejected for Potassium (K) and Iron (Fe) (see table \ref{tab:KS-type1}) and for type 2 glass $H_0$ can only be rejected for Iron (Fe) (see table \ref{tab:KS-type2}), the tests on type 7 glass show that for none of the contained elements the null hypothesis can be rejected at the 1\, \% significance level.


\begin{table}[h!]
\centering
<<testresKSType1, echo=FALSE, term=TRUE, results=tex>>=
cat("\\begin{tabular}{|cccccc|} ")
cat("\\hline ")
cat(paste("variable & test statistic & sig. level & critical value & p-value & rejected\\\\ ", "\n", sep=""))
cat("\\hline ")
tr=testresultKS.full

for(i in 1:(length(tr))) {
 
  cat(paste(
    names(tr[i]),
    round(tr[i][[1]]$teststat, 2),
    0.01,
    round(tr[i][[1]]$critical_value, 2),
    ifelse(tr[i][[1]]$p_value < 1e-15, "< 1.0e-15", tr[i][[1]]$p_value),
    ifelse(tr[i][[1]]$rejected, "yes", "no"),
           sep=" & "), "\\\\ ", "\n", sep="")
}


cat("\\hline ")
cat(paste("\\end{tabular}", "\n", sep=""))
@
\caption{Test results of the improved KS test on the whole data sample}
\label{tab:KS-full}
\end{table}


\begin{table}[h!]
\centering
<<testresKSType1, echo=FALSE, term=TRUE, results=tex>>=
cat("\\begin{tabular}{|cccccc|} ")
cat("\\hline ")
cat(paste("variable & test statistic & sig. level & critical value & p-value & rejected\\\\ ", "\n", sep=""))
cat("\\hline ")
tr=testresultKS.type1

for(i in 1:(length(tr))) {
 
  cat(paste(
    names(tr[i]),
    round(tr[i][[1]]$teststat, 2),
    0.01,
    round(tr[i][[1]]$critical_value, 2),
    ifelse(tr[i][[1]]$p_value < 1e-15, "< 1.0e-15", tr[i][[1]]$p_value),
    ifelse(tr[i][[1]]$rejected, "yes", "no"),
           sep=" & "), "\\\\ ", "\n", sep="")
}

cat("\\hline ")
cat(paste("\\end{tabular}", "\n", sep=""))
@
\caption{Test results of the improved KS test on type 1 glass}
\label{tab:KS-type1}
\end{table}



\begin{table}[h!]
\centering
<<testresKSType2, echo=FALSE, term=TRUE, results=tex>>=
cat("\\begin{tabular}{|cccccc|} ")
cat("\\hline ")
cat(paste("variable & test statistic & sig. level & critical value & p-value & rejected\\\\ ", "\n", sep=""))
cat("\\hline ")
tr=testresultKS.type2

for(i in 1:(length(tr))) {
  cat(paste(
    names(tr[i]),
    round(tr[i][[1]]$teststat, 2),
    0.01,
    round(tr[i][[1]]$critical_value, 2),
    ifelse(tr[i][[1]]$p_value < 1e-15, "< 1.0e-15", tr[i][[1]]$p_value),
    ifelse(tr[i][[1]]$rejected, "yes", "no"),
           sep=" & "), "\\\\ ", "\n", sep="")
}

cat("\\hline ")
cat(paste("\\end{tabular}", "\n", sep=""))
@
\caption{Test results of the improved KS test on type 2 glass}
\label{tab:KS-type2}
\end{table}



\begin{table}[h!]
\centering
<<testresKSType7, echo=FALSE, term=TRUE, results=tex>>=
cat("\\begin{tabular}{|cccccc|} ")
cat("\\hline ")
cat(paste("variable & test statistic & sig. level & critical value & p-value & rejected\\\\ ", "\n", sep=""))
cat("\\hline ")
tr=testresultKS.type7

for(i in 1:(length(tr))) {
  cat(paste(
    names(tr[i]),
    round(tr[i][[1]]$teststat, 2),
    0.01,
    round(tr[i][[1]]$critical_value, 2),
    ifelse(tr[i][[1]]$p_value < 1e-15, "< 1.0e-15", tr[i][[1]]$p_value),
    ifelse(tr[i][[1]]$rejected, "yes", "no"),
           sep=" & "), "\\\\ ", "\n", sep="")
}

cat("\\hline ")
cat(paste("\\end{tabular}", "\n", sep=""))
@
\caption{Test results of the improved KS test on type 7 glass}
\label{tab:KS-type7}
\end{table}

\subsection{Testing transformed data}\label{sec:testing-transformed-data}

The same tests are now conducted on the data that have been Box-Cox-transformed with a parameter that is estimated by the maximum likelihood method. For some variables, an estimation is not possible because the algorithm does not converge or, as in most cases, not all of the observations of one variable are strictly positive.
\subsubsection{Q-Q-plot}\label{sec:qq-transformed}

Before testing statistically, the results of the transformation can be examined graphically by again plotting the quantiles of the transformed data and comparing these plots with the QQ-plots from the original data in section \ref{sec:qq-original}. For some of the elements in the dataset the transformation leads to a slight approximation towards normality (figure \ref{fig:QQtrans}). However, for rather complicated cases (e. g. where several distributions seem to be combined) the transformation appearently does not produce normally distributed data (figure \ref{fig:QQtransno}). In these cases the initial distribution is not unimodal and therefore the Box-Cox-transformation is not very helpful (see section \ref{sec:boxcox-theory} for a detailed explanation).

\begin{figure}[h!]
\centering
<<QQtrans, eval=TRUE, echo=FALSE, term=FALSE, fig=TRUE, include=FALSE>>=
Glass.t <- transformDataFrame(Glass[,-10])
Glass.type1s.t <- transformDataFrame(Glass.type1s)
Glass.type2s.t <- transformDataFrame(Glass.type2s)
Glass.type3s.t <- transformDataFrame(Glass.type3s)
Glass.type5s.t <- transformDataFrame(Glass.type5s)
Glass.type6s.t <- transformDataFrame(Glass.type6s)
Glass.type7s.t <- transformDataFrame(Glass.type7s)

par(mfrow=c(2,2))
qqnorm(Glass.type1s[["Na"]], main ="Na Glass Type 1",pch=19,
       cex.lab=1)
qqline(Glass.type1s[["Na"]],lwd=2,col="red")
qqnorm(Glass.type1s.t[["Na"]], main ="Na Glass Type 1 transformed",pch=19,
       cex.lab=1)
qqline(Glass.type1s.t[["Na"]],lwd=2,col="red")
qqnorm(Glass.type7s[["Ca"]], main ="Ca Glass Type 7",pch=19,
       cex.lab=1)
qqline(Glass.type7s[["Ca"]],lwd=2,col="red")
qqnorm(Glass.type7s.t[["Ca"]], main ="Ca Glass Type 7 transformed",pch=19,
       cex.lab=1)
qqline(Glass.type7s.t[["Ca"]],lwd=2,col="red")
par(mfrow=c(1,1))
@
\includegraphics[width=\textwidth]{report-QQtrans}
\caption{QQ-Plots of the cases where the transformation shows slight approximations to normality}
\label{fig:QQtrans}
\end{figure}

\begin{figure}[h!]
\centering
<<QQtransno, eval=TRUE, echo=FALSE, term=FALSE, fig=TRUE, include=FALSE>>=

par(mfrow=c(2,2))
qqnorm(Glass.type1s[["RI"]], main ="RI Glass Type 1",pch=19,
       cex.lab=1)
qqline(Glass.type1s[["RI"]],lwd=2,col="red")
qqnorm(Glass.type1s.t[["RI"]], main ="RI Glass Type 1 transformed",pch=19,
       cex.lab=1)
qqline(Glass.type1s.t[["RI"]],lwd=2,col="red")
qqnorm(Glass.type1s[["Si"]], main ="Si Glass Type 1",pch=19,
       cex.lab=1)
qqline(Glass.type1s[["Si"]],lwd=2,col="red")
qqnorm(Glass.type1s.t[["Si"]], main ="Si Glass Type 1 transformed",pch=19,
       cex.lab=1)
qqline(Glass.type1s.t[["Si"]],lwd=2,col="red")
@
\includegraphics[width=\textwidth]{report-QQtransno}
\caption{QQ-Plots of the non-unimodal cases where the transformation does not lead to normal distributions}
\label{fig:QQtransno}
\end{figure}


\subsubsection{Shapiro-Wilk test}\label{sec:shapiro-transformed}

When performing the Shapiro-Wilk test on the whole transformed data, at the first glance the transformation does not seem to have had the desired impact on the data. As displayed in table \ref{tab:testresFullSWtrans} for all elements the Null Hypothesis is rejected. Although the p-values increased, they are still below the 1\, \% significance level, so there is no fundamental difference to the results from testing the original data.

However, proceeding analogously to section \ref{sec:shapiro-original} and testing the different glass types separately, these separate results reveal some improvements.

Glass type 1 shows the largest number of improvements, since in the original dataset the Null hypothesis was rejected for all variables. The test results of the transformed type 1 data from table \ref{tab:testrestype1SWtrans} show significantly increased p-values. For the three variables Sodium (Na), Aluminium (Al), and Calcium (Ca) now a normal distribution cannot be rejected any more.

For the glass types 2 and 3 there are no changes in the number of rejections at the 1\, \% significance level (tables \ref{tab:testrestype2SWtrans} and \ref{tab:testrestype3SWtrans}). In fact the the p-values of the variables increased considerably with the transformation, however, they still lie below the alpha level.

The test on the transformed glass type 5 data shows that, in addition to refractive index (RI), Sodium (Na), Silicon (Si), and Calcium (Ca), now for Aluminium (Al) and Potassium (K) a normal distribution cannot be rejected any more (see table \ref{tab:testrestype6SWtrans}).

Furthermore for glass type 6 the p-values increase significantly as well, except for Silicon (Si) which only slightly increases (see table \ref{tab:testrestype6SWtrans}). Consequently, $H_0$ is, analogously to the original data, not rejected for Aluminium (Al), Silicon (Si), and Calcium (Ca), plus the refractive index (RI) for which $H_0$ cannot be rejected after the transformation.

The test results of the transformed glass type 7 data show that for Aluminium (Al) the Null hypothesis still cannot be rejected. Due to contained zero-values Barium (Ba) is excluded from the transformation (see section \ref{sec:testing-transformed-data}) and therefore nothing is concluded about this specific variable. Furthermore, the result shows that the p-values of Sodium (Na) and Calcium (Ca) considerably increase and exceed the 1\, \% significance level after the transformation.

In a nutshell, the Shapiro-Wilk test shows shows significant improvements towards a normal distribution of the variables when the Box-Cox-transformation is used and the different glass types are analysed separately. Analysing the full dataset also shows some minor improvements of the p-values, however, $H_0$ is still rejected at the chosen significance level. An explanation for this result could be that in the full dataset different distributions of the different distinctive glass types are combined and therefore the Box-Cox-transformation does not improve the distribution (see section \ref{sec:boxcox-theory}).


<<calcSWresultstrans, echo=FALSE, term=TRUE>>=

shapresult.full.t <- sapply(colnames(Glass.t[,-10]), function(x) {
  if (is.na(Glass.t[[x]][1])) {
  }
  else {
    shapiro.test(Glass.t[[x]])
  }
})

shapresult.type1.t <- sapply(colnames(Glass.type1s.t), function(x) {
  if (is.na(Glass.type1s.t[[x]][1])) {
  }
  else {
    shapiro.test(Glass.type1s.t[[x]])
  }
})

shapresult.type2.t <- sapply(colnames(Glass.type2s.t), function(x) {
  if (is.na(Glass.type2s.t[[x]][1])) {
  }
  else {
    shapiro.test(Glass.type2s.t[[x]])
  }
})

shapresult.type3.t <- sapply(colnames(Glass.type3s.t), function(x) {
  if (is.na(Glass.type3s.t[[x]][1])) {
  }
  else {
    shapiro.test(Glass.type3s.t[[x]])
  }
})

shapresult.type5.t <- sapply(colnames(Glass.type5s.t), function(x) {
  if (is.na(Glass.type5s.t[[x]][1])) {
  }
  else {
    shapiro.test(Glass.type5s.t[[x]])
  }
})

shapresult.type6.t <- sapply(colnames(Glass.type6s.t), function(x) {
  if (is.na(Glass.type6s.t[[x]][1])) {
  }
  else {
    shapiro.test(Glass.type6s.t[[x]])
  }
})

shapresult.type7.t <- sapply(colnames(Glass.type7s.t), function(x) {
  if (is.na(Glass.type7s.t[[x]][1])) {
  }
  else {
    shapiro.test(Glass.type7s.t[[x]])
  }
})
@





\subsubsection{Pearson's chi-squared test}
<<calculatChisqTrans, echo=FALSE>>=
testresult.full.trans = apply(transformDataFrame(Glass[, -10]), 2, chisq.test.norm.eh, initial_breaks=10, min=5, sig=0.01)
testresult.type1.trans = apply(transformDataFrame(Glass.type1[, c(-8,-10)]), 2, chisq.test.norm.eh, initial_breaks=10, min=5, sig=0.01)
testresult.type2.trans = apply(transformDataFrame(Glass.type2[, c(-8,-10)]), 2, chisq.test.norm.eh, initial_breaks=10, min=5, sig=0.01)
testresult.type7.trans = apply(transformDataFrame(Glass.type7[, c(-3,-9)]), 2, chisq.test.norm.eh, initial_breaks=10, min=5, sig=0.01)
@
Although for all variables for which a transformation is possible the p-value is higher than for the non-transformed data, the hypothesis of normality is still rejected for the whole sample (table \ref{tab:chi-full-trans}). The data of all types of glass is presumably too heterogenuous so that it comprises samples from several distributions within the overall sample of particular variables.

Concerning the transformation of type 1 glass, for two more variables (Al and Ca) the hypothesis of normality now cannot be rejected (table \ref{tab:chi-type1-trans}). In both cases, the p-value increases substantially through the Box-Cox-transformation. For the variable Calcium, the frequencies of the original data are slightly shifted to lower values (figure \ref{fig:chisqType1Ca}) whereas the transformation fits the data approximately to an according normal distribution (figure \ref{fig:chisqType1CaTrans}).

\begin{figure}[h!]
\centering
<<chisqType1Ca, eval=TRUE, echo=FALSE, term=FALSE, fig=TRUE, include=FALSE>>=
testres = chisq.test.norm(Glass.type1$Ca, 10, 5, 0.05)
hist=testres$hist
hist.exp=histFromExpectedFreqs(testres)
plot(hist, col=rgb(1, 0, 0,0.5), main="Histogram of observed and expected densities", xlab="amount of Calcium in weight percent")
plot(hist.exp, col=rgb(0, 0, 1,0.5), add=TRUE)
@
\includegraphics[width=0.8\textwidth]{report-chisqType1Ca}
\caption{Histogram of observed densities (red) and expected densities (blue) within the classes for the variable Calcium of type 1 glass}
\label{fig:chisqType1Ca}
\end{figure}

\begin{figure}[h!]
\centering
<<chisqType1CaTrans, eval=TRUE, echo=FALSE, term=FALSE, fig=TRUE, include=FALSE>>=
testres = chisq.test.norm(transformVector(Glass.type1$Ca), 10, 5, 0.05)
hist=testres$hist
hist.exp=histFromExpectedFreqs(testres)
plot(hist, col=rgb(1, 0, 0,0.5), main="Histogram of observed and expected densities", xlab="transformed amount of Calcium in weight percent")
plot(hist.exp, col=rgb(0, 0, 1,0.5), add=TRUE)
@
\includegraphics[width=0.8\textwidth]{report-chisqType1CaTrans}
\caption{Histogram of observed densities (red) and expected densities (blue) within the classes for transformed values of the variable Calcium of type 1 glass}
\label{fig:chisqType1CaTrans}
\end{figure}



Similar effects can be observed for the results of type 2 glass (table \ref{tab:chi-type2-trans}). Here, the hypothesis of normality is additionally not rejected for the variable Sodium.


For the observations of type 7 glass, the test can now even be conducted on data of the variable Sodium, which were not properly distributed to perform the test before, \ie the data were not divided into a sufficient number of classes (table \ref{tab:chi-type7-trans}).

\subsubsection{Kolmogorov-Smirnov test}

%<<calculatKSTrans, eval=TRUE, term=TRUE ,echo=FALSE>>=
%testresultKS.full.trans = apply(transformDataFrame(Glass[, -10]), 2, KSimpr.test.norm, sig=0.01)
%@
Since in the KS test of the original dataset the null hypotheses was only rejected for the variables Magnesium (Mg), Potassium (K), Barium (Ba), and Iron (Fe), transforming the data would aim at achieving a normal distribution for these four variables. However, exactly these four the transformation is not possible (see section \ref{sec:testing-transformed-data}). Therefore KS testing the transformed data will not provide further insights.

%The results are shown in table \ref{tab:KS-full}.

%\begin{table}[h!]
%\centering
%<<testresKSFullTrans, echo=FALSE, term=TRUE, results=tex>>=
%cat("\\begin{tabular}{|cccccc|} ")
%cat("\\hline ")
%cat(paste("variable & test statistic & sig. level & critical value & p-value & rejected\\\\ ", "\n", sep=""))
%cat("\\hline ")
%tr=testresultKS.full.trans
%
%for(i in 1:(length(tr))) {
%  cat(paste(
%    names(tr[i]),
%    round(tr[i][[1]]$teststat, 2),
%    0.01,
%    round(tr[i][[1]]$critical_value, 2),
%    ifelse(tr[i][[1]]$p_value < 1e-15, "< 1.0e-15", tr[i][[1]]$p_value),
%    ifelse(tr[i][[1]]$rejected, "yes", "no"),
%           sep=" & "), "\\\\ ", "\n", sep="")
%}
%
%cat("\\hline ")
%cat(paste("\\end{tabular}", "\n", sep=""))
%@
%\caption{Test results of the improved KS test on the whole transformed data
%sample with ten initial classes}
%\label{tab:KS-fullTrans}
%\end{table}
\subsection{Contour plots of selected variables}

The search for bivariate normally distributed variables shall be now demonstrated by an example from the Glass sample. Concerning the complete sample, the p-values for the variables Na and Al are highest among all used test methods. Therefore, these two variables are selected for the contour plot (\ref{fig:contourGlassNaAl}).

\begin{figure}[h!]
<<contourGlassNaAl, eval=TRUE, echo=FALSE, term=FALSE, fig=TRUE, include=FALSE>>=
ci2d(Glass$Na, Glass$Al, show="contour", col="black", show.points=TRUE, factor=3)
@
\centering
\includegraphics[width=0.6\textwidth]{report-contourGlassNaAl}
\caption{Data plot with contour lines of the variables Na and Al of the whole sample}
\label{fig:contourGlassNaAl}
\end{figure}


From the shape of the plot, no clear insight can be deducted about multivariate normality. In the center, the contour lines have an elliptical shape whereas an accumulation of data points can be observed at the upper right corner. This accumulation is presumably due to different distributions of variables among particular glass types. Indeed, it can be ascribed to the data of type 7 glass, which can be seen in figure \ref{fig:cluster7}.

\begin{figure}[h!]
<<cluster7, eval=TRUE, echo=FALSE, term=FALSE, fig=TRUE, include=FALSE>>=
plot(Glass$Na, Glass$Al, xlim=c(9.9,18.22), ylim=c(-0.12,3.9))
points(Glass[which(Glass$Type == 7), -10]$Na, Glass[which(Glass$Type == 7), -10]$Al, col="red", pch=19)
@
\centering
\includegraphics[width=0.6\textwidth]{report-cluster7}
\caption{Data plot of the variables Na and Al of the whole sample. Data points of type 7 glass are highlighted in red.}
\label{fig:cluster7}
\end{figure}

If the same plot is drawn for the whole sample without data of type 7 glass (figure \ref{fig:contourGlassNaAl-7}), an elliptical shape is recognisable, except for some outliers. Quantitative tests should be conducted next in order to make a more accurate point about the compliance of the data with a multivariate normal distribution.

\begin{figure}[h!]
<<contourGlassNaAl-7, eval=TRUE, echo=FALSE, term=FALSE, fig=TRUE, include=FALSE>>=
ci2d(Glass[which(Glass$Type != 7), ]$Na, Glass[which(Glass$Type != 7), ]$Al, show="contour", col="black", show.points=TRUE, factor=3)
@
\centering
\includegraphics[width=0.6\textwidth]{report-contourGlassNaAl-7}
\caption{Data plot with contour lines of the variables Na and Al of the whole sample except data of type 7 glass}
\label{fig:contourGlassNaAl-7}
\end{figure}


\newpage
\section{Conclusion}\label{sec:conclusion}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% appendix

\newpage

\begin{appendix}
\pagenumbering{roman}		%% roman page numbers for the appendix



\section{Appendix}

\begin{table}[h!]
\centering
<<testresFull, echo=FALSE, term=TRUE, results=tex>>=
cat("\\begin{tabular}{|cccccc|} ")
cat("\\hline ")
cat(paste("variable & test statistic & sig. level & critical value & p-value & rejected\\\\ ", "\n", sep=""))
cat("\\hline ")
tr=testresult.full.30

for(i in 1:(length(tr))) {
  cat(paste(
    names(tr[i]),
    round(tr[i][[1]]$teststat, 2),
    0.01,
    round(tr[i][[1]]$critical_value, 2),
    ifelse(tr[i][[1]]$p_value < 1e-15, "< 1.0e-15", tr[i][[1]]$p_value),
    ifelse(tr[i][[1]]$rejected, "yes", "no"),
           sep=" & "), "\\\\ ", "\n", sep="")
}

cat("\\hline ")
cat(paste("\\end{tabular}", "\n", sep=""))
@
\caption{Test results of the chi-squared test on the whole data sample with 30 initial classes}
\label{tab:chi-full-30}
\end{table}

\begin{table}[h!]
\centering
<<testresType2-30, echo=FALSE, term=TRUE, results=tex>>=
cat("\\begin{tabular}{|cccccc|} ")
cat("\\hline ")
cat(paste("variable & test statistic & sig. level & critical value & p-value & rejected\\\\ ", "\n", sep=""))
cat("\\hline ")
tr=testresult.type1.30

for(i in 1:(length(tr))) {
  cat(paste(
    names(tr[i]),
    round(tr[i][[1]]$teststat, 2),
    0.01,
    round(tr[i][[1]]$critical_value, 2),
    ifelse(tr[i][[1]]$p_value < 1e-15, "< 1.0e-15", tr[i][[1]]$p_value),
    ifelse(tr[i][[1]]$rejected, "yes", "no"),
           sep=" & "), "\\\\ ", "\n", sep="")
}

cat("\\hline ")
cat(paste("\\end{tabular}", "\n", sep=""))
@
\caption{Test results of the chi-squared test on type 1 glass with 30 initial classes}
\label{tab:chi-type1-30}
\end{table}

\begin{table}[h!]
\centering
<<testresType2-30, echo=FALSE, term=TRUE, results=tex>>=
cat("\\begin{tabular}{|cccccc|} ")
cat("\\hline ")
cat(paste("variable & test statistic & sig. level & critical value & p-value & rejected\\\\ ", "\n", sep=""))
cat("\\hline ")
tr=testresult.type2.30

for(i in 1:(length(tr))) {
  cat(paste(
    names(tr[i]),
    round(tr[i][[1]]$teststat, 2),
    0.01,
    round(tr[i][[1]]$critical_value, 2),
    ifelse(tr[i][[1]]$p_value < 1e-15, "< 1.0e-15", tr[i][[1]]$p_value),
    ifelse(tr[i][[1]]$rejected, "yes", "no"),
           sep=" & "), "\\\\ ", "\n", sep="")
}

cat("\\hline ")
cat(paste("\\end{tabular}", "\n", sep=""))
@
\caption{Test results of the chi-squared test on type 2 glass with 30 initial classes}
\label{tab:chi-type2-30}
\end{table}

\begin{table}[h!]
\centering
<<testresType7-30, echo=FALSE, term=TRUE, results=tex>>=
cat("\\begin{tabular}{|cccccc|} ")
cat("\\hline ")
cat(paste("variable & test statistic & sig. level & critical value & p-value & rejected\\\\ ", "\n", sep=""))
cat("\\hline ")
tr=testresult.type7.30

for(i in 1:(length(tr))) {
  cat(paste(
    names(tr[i]),
    round(tr[i][[1]]$teststat, 2),
    0.01,
    round(tr[i][[1]]$critical_value, 2),
    ifelse(tr[i][[1]]$p_value < 1e-15, "< 1.0e-15", tr[i][[1]]$p_value),
    ifelse(tr[i][[1]]$rejected, "yes", "no"),
           sep=" & "), "\\\\ ", "\n", sep="")
}

cat("\\hline ")
cat(paste("\\end{tabular}", "\n", sep=""))
@
\caption{Test results of the chi-squared test on type 7 glass with 30 initial classes}
\label{tab:chi-type7-30}
\end{table}

\begin{table}[h!]
\centering
<<testresFullSWtrans, echo=FALSE, term=TRUE, results=tex>>=
cat("\\begin{tabular}{|cccccc|} ")
cat("\\hline ")
cat(paste("variable & test statistic & sig. level & critical value & p-value & rejected\\\\ ", "\n", sep=""))
cat("\\hline ")
tr=shapresult.full.t

for(i in 1:(length(tr))) {
  varname = names(tr[i])
  if (is.null(tr[[varname]])) {
    cat(paste(varname, "NA", "NA", "NA", "NA", "NA", sep=" & "))
    cat(paste("\\\\ ", "\n", sep=""))
  }
  else {
    pvalue = tr[[varname]][["p.value"]]
    cat(paste(
      varname,
      round(tr[[varname]][["statistic"]], 2),
      sig,
      "NA",
      ifelse(pvalue < 1e-15, "< 1.0e-15", pvalue),
      ifelse(pvalue<=sig, "yes", "no"),
             sep=" & "), "\\\\ ", "\n", sep="")
  }
}

cat("\\hline ")
cat(paste("\\end{tabular}", "\n", sep=""))
@
\caption{Test results of the Shapiro-Wilk test on the whole transformed data sample}
\label{tab:testresFullSWtrans}
\end{table}

\begin{table}[h!]
\centering
<<testrestype1SWtrans, echo=FALSE, term=TRUE, results=tex>>=
cat("\\begin{tabular}{|cccccc|} ")
cat("\\hline ")
cat(paste("variable & test statistic & sig. level & critical value & p-value & rejected\\\\ ", "\n", sep=""))
cat("\\hline ")
tr=shapresult.type1.t

for(i in 1:(length(tr))) {
  varname = names(tr[i])
  if (is.null(tr[[varname]])) {
    cat(paste(varname, "NA", "NA", "NA", "NA", "NA", sep=" & "))
    cat(paste("\\\\ ", "\n", sep=""))
  }
  else {
    pvalue = tr[[varname]][["p.value"]]
    cat(paste(
      varname,
      round(tr[[varname]][["statistic"]], 2),
      sig,
      "NA",
      ifelse(pvalue < 1e-15, "< 1.0e-15", pvalue),
      ifelse(pvalue<=sig, "yes", "no"),
             sep=" & "), "\\\\ ", "\n", sep="")
  }
}

cat("\\hline ")
cat(paste("\\end{tabular}", "\n", sep=""))
@
\caption{Test results of the Shapiro-Wilk test on the transformed type 1 glass}
\label{tab:testrestype1SWtrans}
\end{table}

\begin{table}[h!]
\centering
<<testrestype2SWtrans, echo=FALSE, term=TRUE, results=tex>>=
cat("\\begin{tabular}{|cccccc|} ")
cat("\\hline ")
cat(paste("variable & test statistic & sig. level & critical value & p-value & rejected\\\\ ", "\n", sep=""))
cat("\\hline ")
tr=shapresult.type2.t

for(i in 1:(length(tr))) {
  varname = names(tr[i])
  if (is.null(tr[[varname]])) {
    cat(paste(varname, "NA", "NA", "NA", "NA", "NA", sep=" & "))
    cat(paste("\\\\ ", "\n", sep=""))
  }
  else {
    pvalue = tr[[varname]][["p.value"]]
    cat(paste(
      varname,
      round(tr[[varname]][["statistic"]], 2),
      sig,
      "NA",
      ifelse(pvalue < 1e-15, "< 1.0e-15", pvalue),
      ifelse(pvalue<=sig, "yes", "no"),
             sep=" & "), "\\\\ ", "\n", sep="")
  }
}

cat("\\hline ")
cat(paste("\\end{tabular}", "\n", sep=""))
@
\caption{Test results of the Shapiro-Wilk test on the transformed type 2 glass}
\label{tab:testrestype2SWtrans}
\end{table}

\begin{table}[h!]
\centering
<<testrestype3SWtrans, echo=FALSE, term=TRUE, results=tex>>=
cat("\\begin{tabular}{|cccccc|} ")
cat("\\hline ")
cat(paste("variable & test statistic & sig. level & critical value & p-value & rejected\\\\ ", "\n", sep=""))
cat("\\hline ")
tr=shapresult.type3.t

for(i in 1:(length(tr))) {
  varname = names(tr[i])
  if (is.null(tr[[varname]])) {
    cat(paste(varname, "NA", "NA", "NA", "NA", "NA", sep=" & "))
    cat(paste("\\\\ ", "\n", sep=""))
  }
  else {
    pvalue = tr[[varname]][["p.value"]]
    cat(paste(
      varname,
      round(tr[[varname]][["statistic"]], 2),
      sig,
      "NA",
      ifelse(pvalue < 1e-15, "< 1.0e-15", pvalue),
      ifelse(pvalue<=sig, "yes", "no"),
             sep=" & "), "\\\\ ", "\n", sep="")
  }
}

cat("\\hline ")
cat(paste("\\end{tabular}", "\n", sep=""))
@
\caption{Test results of the Shapiro-Wilk test on the transformed type 3 glass}
\label{tab:testrestype3SWtrans}
\end{table}

\begin{table}[h!]
\centering
<<testrestype5SWtrans, echo=FALSE, term=TRUE, results=tex>>=
cat("\\begin{tabular}{|cccccc|} ")
cat("\\hline ")
cat(paste("variable & test statistic & sig. level & critical value & p-value & rejected\\\\ ", "\n", sep=""))
cat("\\hline ")
tr=shapresult.type5.t

for(i in 1:(length(tr))) {
  varname = names(tr[i])
  if (is.null(tr[[varname]])) {
    cat(paste(varname, "NA", "NA", "NA", "NA", "NA", sep=" & "))
    cat(paste("\\\\ ", "\n", sep=""))
  }
  else {
    pvalue = tr[[varname]][["p.value"]]
    cat(paste(
      varname,
      round(tr[[varname]][["statistic"]], 2),
      sig,
      "NA",
      ifelse(pvalue < 1e-15, "< 1.0e-15", pvalue),
      ifelse(pvalue<=sig, "yes", "no"),
             sep=" & "), "\\\\ ", "\n", sep="")
  }
}

cat("\\hline ")
cat(paste("\\end{tabular}", "\n", sep=""))
@
\caption{Test results of the Shapiro-Wilk test on the transformed type 5 glass}
\label{tab:testrestype5SWtrans}
\end{table}

\begin{table}[h!]
\centering
<<testrestype6SWtrans, echo=FALSE, term=TRUE, results=tex>>=
cat("\\begin{tabular}{|cccccc|} ")
cat("\\hline ")
cat(paste("variable & test statistic & sig. level & critical value & p-value & rejected\\\\ ", "\n", sep=""))
cat("\\hline ")
tr=shapresult.type6.t

for(i in 1:(length(tr))) {
  varname = names(tr[i])
  if (is.null(tr[[varname]])) {
    cat(paste(varname, "NA", "NA", "NA", "NA", "NA", sep=" & "))
    cat(paste("\\\\ ", "\n", sep=""))
  }
  else {
    pvalue = tr[[varname]][["p.value"]]
    cat(paste(
      varname,
      round(tr[[varname]][["statistic"]], 2),
      sig,
      "NA",
      ifelse(pvalue < 1e-15, "< 1.0e-15", pvalue),
      ifelse(pvalue<=sig, "yes", "no"),
             sep=" & "), "\\\\ ", "\n", sep="")
  }
}

cat("\\hline ")
cat(paste("\\end{tabular}", "\n", sep=""))
@
\caption{Test results of the Shapiro-Wilk test on the transformed type 6 glass}
\label{tab:testrestype6SWtrans}
\end{table}

\begin{table}[h!]
\centering
<<testrestype7SWtrans, echo=FALSE, term=TRUE, results=tex>>=
cat("\\begin{tabular}{|cccccc|} ")
cat("\\hline ")
cat(paste("variable & test statistic & sig. level & critical value & p-value & rejected\\\\ ", "\n", sep=""))
cat("\\hline ")
tr=shapresult.type7.t

for(i in 1:(length(tr))) {
  varname = names(tr[i])
  if (is.null(tr[[varname]])) {
    cat(paste(varname, "NA", "NA", "NA", "NA", "NA", sep=" & "))
    cat(paste("\\\\ ", "\n", sep=""))
  }
  else {
    pvalue = tr[[varname]][["p.value"]]
    cat(paste(
      varname,
      round(tr[[varname]][["statistic"]], 2),
      sig,
      "NA",
      ifelse(pvalue < 1e-15, "< 1.0e-15", pvalue),
      ifelse(pvalue<=sig, "yes", "no"),
             sep=" & "), "\\\\ ", "\n", sep="")
  }
}

cat("\\hline ")
cat(paste("\\end{tabular}", "\n", sep=""))
@
\caption{Test results of the Shapiro-Wilk test on the transformed type 7 glass}
\label{tab:testrestype7SWtrans}
\end{table}

\begin{table}[h!]
\centering
<<testresFullTrans, echo=FALSE, term=TRUE, results=tex>>=
cat("\\begin{tabular}{|cccccc|} ")
cat("\\hline ")
cat(paste("variable & test statistic & sig. level & critical value & p-value & rejected\\\\ ", "\n", sep=""))
cat("\\hline ")
tr=testresult.full.trans

for(i in 1:(length(tr))) {
  cat(paste(
    names(tr[i]),
    round(tr[i][[1]]$teststat, 2),
    0.01,
    round(tr[i][[1]]$critical_value, 2),
    ifelse(is.na(tr[i][[1]]$p_value), NA, ifelse(tr[i][[1]]$p_value < 1e-15, "< 1.0e-15", tr[i][[1]]$p_value)),
    ifelse(is.na(tr[i][[1]]$rejected), NA, ifelse(tr[i][[1]]$rejected, "yes", "no")),
           sep=" & "), "\\\\ ", "\n", sep="")
}

cat("\\hline ")
cat(paste("\\end{tabular}", "\n", sep=""))
@
\caption{Test results of the chi-squared test on the whole transformed data sample with ten initial classes}
\label{tab:chi-full-trans}
\end{table}

\begin{table}[h!]
\centering
<<testresType1Trans, echo=FALSE, term=TRUE, results=tex>>=
cat("\\begin{tabular}{|cccccc|} ")
cat("\\hline ")
cat(paste("variable & test statistic & sig. level & critical value & p-value & rejected\\\\ ", "\n", sep=""))
cat("\\hline ")
tr=testresult.type1.trans

for(i in 1:(length(tr))) {
  cat(paste(
    names(tr[i]),
    round(tr[i][[1]]$teststat, 2),
    0.01,
    round(tr[i][[1]]$critical_value, 2),
    ifelse(is.na(tr[i][[1]]$p_value), NA, ifelse(tr[i][[1]]$p_value < 1e-15, "< 1.0e-15", tr[i][[1]]$p_value)),
    ifelse(is.na(tr[i][[1]]$rejected), NA, ifelse(tr[i][[1]]$rejected, "yes", "no")),
           sep=" & "), "\\\\ ", "\n", sep="")
}

cat("\\hline ")
cat(paste("\\end{tabular}", "\n", sep=""))
@
\caption{Test results of the chi-squared test on the transformed data of type 1 glass with ten initial classes}
\label{tab:chi-type1-trans}
\end{table}

\begin{table}[h!]
\centering
<<testresType2Trans, echo=FALSE, term=TRUE, results=tex>>=
cat("\\begin{tabular}{|cccccc|} ")
cat("\\hline ")
cat(paste("variable & test statistic & sig. level & critical value & p-value & rejected\\\\ ", "\n", sep=""))
cat("\\hline ")
tr=testresult.type2.trans

for(i in 1:(length(tr))) {
  cat(paste(
    names(tr[i]),
    round(tr[i][[1]]$teststat, 2),
    0.01,
    round(tr[i][[1]]$critical_value, 2),
    ifelse(is.na(tr[i][[1]]$p_value), NA, ifelse(tr[i][[1]]$p_value < 1e-15, "< 1.0e-15", tr[i][[1]]$p_value)),
    ifelse(is.na(tr[i][[1]]$rejected), NA, ifelse(tr[i][[1]]$rejected, "yes", "no")),
           sep=" & "), "\\\\ ", "\n", sep="")
}

cat("\\hline ")
cat(paste("\\end{tabular}", "\n", sep=""))
@
\caption{Test results of the chi-squared test on the transformed data of type 2 glass with ten initial classes}
\label{tab:chi-type2-trans}
\end{table}

\begin{table}[h!]
\centering
<<testresType7Trans, echo=FALSE, term=TRUE, results=tex>>=
cat("\\begin{tabular}{|cccccc|} ")
cat("\\hline ")
cat(paste("variable & test statistic & sig. level & critical value & p-value & rejected\\\\ ", "\n", sep=""))
cat("\\hline ")
tr=testresult.type7.trans

for(i in 1:(length(tr))) {
  cat(paste(
    names(tr[i]),
    round(tr[i][[1]]$teststat, 2),
    0.01,
    round(tr[i][[1]]$critical_value, 2),
    ifelse(is.na(tr[i][[1]]$p_value), NA, ifelse(tr[i][[1]]$p_value < 1e-15, "< 1.0e-15", tr[i][[1]]$p_value)),
    ifelse(is.na(tr[i][[1]]$rejected), NA, ifelse(tr[i][[1]]$rejected, "yes", "no")),
           sep=" & "), "\\\\ ", "\n", sep="")
}

cat("\\hline ")
cat(paste("\\end{tabular}", "\n", sep=""))
@
\caption{Test results of the chi-squared test on the transformed data of type 7 glass with ten initial classes}
\label{tab:chi-type7-trans}
\end{table}

\end{appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% lists of figures and tables

\newpage
\listoffigures
\listoftables

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% bibliography (if needed)

%\nocite{*}
   \begin{thebibliography}{99}
    \bibitem{ucl} Bache, K., Lichman, M. (2013), UCI Machine Learning Repository, URL: \texttt{http://archive.ics.uci.edu/ml/datasets/Glass+Identification},  Irvine, CA, University of California, School of Information and Computer Science. 
    \bibitem{De02} DeGroot, Morris, H., Mark, J. (2003) Probability and
    Statistics. 3rd ed. Boston, MA: Addison-Wesley.
    \bibitem{steel} Steel, R.G.D., Torrie, J.H. (1960), Principles and Procedures of Statistics with Special Reference to the Biological Sciences, McGraw Hill, p. 350.
    
  \end{thebibliography}
\end{document}
